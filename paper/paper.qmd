---
title: "Red, Blue, or Purple? 2024’s Battleground States Tell a Story"
subtitle: "Pennsylvania, Arizona, and Nevada Become Game Changers as Trump Gains Ground [To be Update]"
author: 
  - Yingke He
  - Ziheng Zhong
thanks: "Code and data are available at: [https://github.com/iJustinn/Election_Prediction](https://github.com/RohanAlexander/starter_folder)."
date: today
date-format: long
abstract: "This paper analyzes polling data for the 2024 U.S. Presidential election, examining the key candidates notabily Donald Trump and Kamala Harris to gain a comprehensive perspective on the race. A Hierarchical Bayesian Model is applied in this study and the findings reveal significant variations across states, with [To be updated...] emerging as particularly influential battlegrounds that ultimately tilt in favor of [To be updated..].By highlighting these trends, this research underscores the dynamic and evolving nature of U.S. political landscapes and the crucial role that swing states play in determining election outcomes."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(kableExtra)
library(ggplot2)
library(dplyr)
library(here)
library(knitr)
library(rstanarm)
library(modelsummary)

#read in cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

cleaned_poll_data <-
  read_csv(file = here("data", "02-analysis_data", "num_grade_pollscore_data.csv"), 
           show_col_types = FALSE)

#model <- stan_glmer(
#  pct ~ (1 | state) + (1 | pollster_rating_name) + candidate_name + sample_size + days_to_election,
#  data = data,
#  family = gaussian(), 
#  prior = normal(0, 2.5),
#  prior_intercept = normal(0, 2.5),
#  prior_aux = exponential(1),
#  iter = 2000, chains = 4
# )
```



# Introduction {#sec-intro}

In the lead-up to the 2024 U.S. Presidential election, understanding voter behavior across states and demographic groups is essential for accurately predicting electoral outcomes. Recent polling data from various sources suggests a close race among major candidates, including Donald Trump, Kamala Harris, and other key contenders. This study applies a Hierarchical Bayesian Model to analyze polling data across all U.S. states, with a focus on identifying critical swing states where even slight shifts in voter sentiment could prove decisive. By exploring trends in voter support across different states and demographic groups, this research aims to highlight the regions most likely to influence the final election outcome.

The primary objective of this study is to estimate state-level vote shares for each candidate while accounting for factors such as regional variations, pollster-specific biases, and the timing of polls relative to Election Day. Predictors including polling percentages, sample sizes, and days remaining until the election are incorporated into the model, which adjusts for state and pollster effects. The model’s structure enables the estimation of vote shares that reflect the underlying distribution of polling data across diverse regions in the U.S.

Initial findings reveal variations in candidate support across states, with certain swing states emerging as pivotal in determining the Electoral College result. The model’s projections suggest a likely lead for Trump in the Electoral College, underscoring the influence of large-sample, recent polls on forecast accuracy. This study’s results emphasize the importance of swing states in the electoral process, demonstrating how regional dynamics and polling methodologies impact predictions.[To Be updated...]

This research contributes to the field of election forecasting by combining aggregated polling data with a robust modeling approach, offering valuable insights for political analysts, campaign strategists, and policymakers. By identifying critical swing states and accounting for potential polling biases, this study equips stakeholders with the tools to anticipate voter shifts and strategize effectively in an evolving political landscape.

The structure of the paper is organized as follows: following @sec-intro, @sec-data outlines the data collection and cleaning process, along with a description of the outcome and predictor variables used in the analysis.@sec-model, introduces the forecasting models and discuss the rationale behind choosing these models for election outcomes prediction. @sec-result then presents the main findings, including a breakdown of state-level and pollster-level random effects. Finally, @sec-discussion interprets the results, highlighting significant trends and predictions, and concludes with a discussion on the reliability of the forecasts and potential limitations of the models.


# Data {#sec-data}

## Overview [info need update]

We use the statistical programming language R [@citeR].... Our data [@shelter].... Following @tellingstories, we consider...

Overview text [To be updated...]

## Data Measurement [info need update]
	
In this study, polling data provides insight into public opinion across different states, measuring support for candidates ahead of the 2024 U.S. Presidential election. The data originates from various reputable polling organizations, each employing distinct methodologies that influence data quality and representativeness. Understanding these methodologies is crucial for interpreting polling data accurately. The primary categories of measurement methods include:

1. Live Telephone Interviews
Some polling organizations, such as Selzer & Co. and Marist College, conduct live telephone interviews, reaching respondents on both landlines and cell phones. This method often provides more accurate and in-depth data because live interviewers can clarify questions, verify demographic details, and increase respondent engagement. However, it is costlier and time-intensive, leading to smaller sample sizes and potentially fewer surveys. In the real world, live telephone interviews offer insights from a cross-section of the population, though they may over-represent older individuals who are more likely to answer landline calls.

2. Interactive Voice Response (IVR)
Pollsters like Trafalgar Group and Rasmussen Reports frequently use IVR, or “robo-polling,” where an automated system asks questions over the phone. IVR is more affordable and efficient for reaching large numbers of people. However, because respondents interact with a machine rather than a live person, they may feel less invested in the survey, potentially impacting response rates and depth of engagement. IVR allows pollsters to obtain timely snapshots of voter intentions, though it may miss out on nuanced responses that a live interviewer could capture.

3. Online Panels
YouGov and Ipsos are known for relying on online panels, which recruit participants via web platforms to answer surveys electronically. This method allows for rapid data collection from a broad audience and is relatively cost-effective. However, online surveys can suffer from self-selection bias, as individuals who are more politically active or internet-savvy are more likely to participate. Despite this, online panels remain popular due to their scalability and cost-efficiency, providing a contemporary approach to measuring public sentiment.

4. Mixed-Mode Surveys (Telephone and Online)
To balance the benefits and limitations of each mode, organizations like The New York Times/Siena College and Public Policy Polling employ mixed-mode surveys. By combining online and telephone interviews, these pollsters aim to broaden demographic reach and mitigate biases inherent in any single method. Mixed-mode surveys improve representativeness and accommodate diverse respondent preferences, providing data that may better reflect the full spectrum of public opinion.

5. Specialized In-Person and University-Affiliated Research
Some university-affiliated pollsters, such as Quinnipiac University and University of New Hampshire Survey Center, incorporate face-to-face interviews or demographic-specific focus groups for deeper insights. These methods can yield highly detailed responses and foster a comprehensive understanding of specific voter groups, though they are resource-intensive and typically used for more specialized research rather than general polling.

These varied data collection methods directly affect how public opinion is measured and interpreted. For instance, live interviews may capture a more detailed view of voter sentiment but often exclude younger demographics who primarily use mobile devices or interact online. Meanwhile, IVR and online panels make large-scale data collection affordable and timely, though they may lack depth and introduce potential self-selection biases. Mixed-mode approaches provide a balanced perspective, catering to both traditional and digital respondents, which may yield more representative data.

The choice of methodology reflects each pollster’s objectives and resources, with implications for accuracy and data interpretation. By categorizing pollsters based on their methodologies, we can contextualize the polling data in this study, recognizing each method's strengths and limitations. This understanding is essential for modeling real-world voter behavior accurately, as it acknowledges how data collection methods impact polling results.

## Outcome variables


According to @tbl-outcome_variables, the outcome variables include critical components for analyzing polling data, such as State, Pollster, Pollscore, Candidate Name, Percentage (pct), Sample Size, Start Date, End Date, and Days to Election. These variables provide the foundation for estimating actual vote shares by capturing regional differences, pollster credibility, candidate-specific support, and timing in relation to Election Day. Together, they ensure a nuanced approach to election forecasting that accounts for variability in poll methodology and voter sentiment shifts over time.
```{r}
#| label: tbl-outcome_variables
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Define the variable names and examples
outcome_variables <- data.frame(
  Variable = c("State", "Pollster", "Pollscore","Candidate Name", "Percentage (pct)", "Sample Size", "Start Date", "End Date", "Days to Election"),
  Example = c("Wisconsin", "RMG Research","0.4" ,"Donald Trump", "48.5%", "789", "2024-9-24", "2024-10-16", "20")
)

# Create the table with an Example column
outcome_variables %>%
  kable(
    col.names = c("Outcome Variable", "Example"),
    caption = "Outcome Variables",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))
```
Above outcome variables @tbl-outcome_variables 


## Predictor variables 
This study includes several key predictor variables that provide insights into regional voting patterns, polling methodologies, and the timing of voter sentiment shifts. Each variable plays a unique role in forecasting vote shares for the 2024 U.S. Presidential election. Below shown in @tbl-predictor_variables are detailed descriptions of each predictor.

```{r}
#| label: tbl-predictor_variables
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Define the variable names
predictor_variables <- data.frame(
  Variable = c("State", "Pollster", "Pollscore","Candidate Name", "Percentage (pct)", "Sample Size", "Days to Election"),
  Example = c("Georgia", "InsiderAdvantage", "0.6","Kamala Harris", "47%", "800", "24")
)

# Create the table
predictor_variables %>%
  kable(
    col.names = c("Predictor Variable", "Example"),
    caption = "Predictor Variables",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))
```

### State

State (`state`) is a categorical variable that represents the U.S. state in which each poll was conducted. States have distinct political dynamics, influenced by regional issues, demographics, and historical voting patterns. Including the state variable allows the model to account for these differences, enabling predictions that are sensitive to each state’s unique electoral landscape.

### Pollster

Pollster (`pollster`) identifies the organization conducting each poll. Different polling organizations may use varied methodologies, such as sample selection, weighting, and question phrasing, which can introduce systematic biases. By including the pollster as a predictor, the model can adjust for potential biases or methodological differences, providing more accurate and standardized vote share estimates.

### Candidate Name 

Candidate Name (`candidate_name`) is a categorical variable that identifies the candidate for whom each poll measures support. Including candidate names allows the model to assess each candidate’s baseline popularity and identify variations in support across different demographics and states. This variable enables comparisons between major contenders, such as Donald Trump and Kamala Harris, as well as other candidates, to forecast overall vote shares.

### Percentage of Votes

Percentage of Votes (`pct`) represents the percentage of respondents in each poll who indicated support for a specific candidate. As a primary predictor, this variable directly informs the model about the current level of support for each candidate at the time the poll was conducted. By converting these percentages into vote share estimates, the model can project likely election outcomes based on current polling data.

### Sample Size

Sample Size (`sample_size`) denotes the number of respondents included in each poll. Larger sample sizes generally increase the reliability of a poll, as they reduce the margin of error and are more likely to represent the broader population. By incorporating sample size as a predictor, the model can weigh polls based on their reliability, giving more importance to larger, more robust samples.

### Pollscore

Pollscore (`pollscore`) variable serves as a measure of each pollster's historical accuracy or quality, reflecting how reliable or unbiased the pollster's results have been over time. A higher pollscore likely indicates a pollster with a better track record of accurate predictions, suggesting their polling methodologies or adjustments may be more robust.

```{r}
#| label: fig-pollscore_distribution
#| echo: false
#| warning: false
#| message: false

# Load necessary libraries
library(tidyverse)
library(here)
library(readr)

# Load the data (assuming `clean_data.csv` contains the poll scores)
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"))

# Plot histogram with overlayed density plot
ggplot(data, aes(x = pollscore)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, fill = "steelblue", color = "white", alpha = 0.7) +
  geom_density(fill = "skyblue", alpha = 0.5) +
  labs(x = "Poll Score",
       y = "Density") +
  theme_minimal()

```

@fig-pollscore_distribution shows the distribution of poll scores The plot shows two primary clusters, with peaks around \(-1\) and near \(0\). This bimodal distribution suggests that pollsters' ratings might tend to fall into two categories, potentially reflecting different methodologies, biases, or levels of reliability among pollsters. For example, scores clustered around \(-1\) could represent pollsters with a more consistent lean or bias in one direction, while scores around \(0\) may indicate neutral or balanced ratings. These clusters highlight the importance of adjusting for pollster reliability when aggregating data for election predictions, as different pollsters contribute varying levels of accuracy and bias. Incorporating such adjustments can enhance the model’s ability to produce a balanced and accurate forecast, providing a more nuanced view of candidate support that takes into account the quality and tendencies of each poll source.

### Days to Election
The variable days to election represents the days remaining from the poll's end date to Election Day.
It is calculated using the `end_date` variable, by subtracting the end_date from the election date, by assuming the election date is November 5 2024.

```{r}

```
days_to_election

# Model {#sec-model}

To predict the actual election vote share for each candidate in each state while accounting for variations by pollster and other poll-specific factors.
Background details and diagnostics are included in [Appendix -@sec-model-details]. [To be updated...]

## Model set-up

A Hierarchical Bayesian Model is utilized to predict the actual election vote for each candidate in each state while accounting for variables by pollster.

The model prediction utilizes the following predictor variables:

- **State** (`state`): Include as a categorical term to capture regional variations.
- **Pollster** (`pollster`): Include as a categorical variable for different polling effects.
- **Pollscore** (`pollscore`): A numerical variable reflecting how reliable the pollster's results have been over time.
- **Candidate Name** (`candidate_name`):Include as a categorical feature or model separately for each candidate.
- **Percentage of Votes by Poll in State** (`pct`): Use as a primary predictor of actual vote share, with a smooth term to allow for non-linear effects.
- **Sample Size** (`sample_size`): Include as a predictor or weight to reflect poll reliability.
- **Days to Election** (`end_date`):capture trends in support leading up to the election.


Let: 
\begin{align*}
y_{ijk} & : \text{ The target variable, representing the actual election vote share for candidate } k \text{ in state } \\
& \quad i \text{ with pollster } j. \\ 
\text{pct}_{ijk} & : \text{ The observed polling percentage for candidate } k \text{ in state } i \text{ by pollster } j. \\ 
\text{sample\_size}_{ijk} & : \text{ The sample size of the poll, which helps in weighing the poll reliability.} \\ 
\text{days\_to\_election}_{ijk} & : \text{ Derived as the days remaining until the election from the poll’s end date, capturing } \\
& \quad \text { the effect of recency.} \\ 
\text{pollscore}_{ijk} & : \text{ The reliability score for each pollster } j, \text{ reflecting the accuracy of the pollster's } \\
& \quad \text { historical results.} \\
\end{align*}

The model takes the form of the following equation:
\begin{align}
y_{ijk} = \alpha_i + \beta_j + \gamma_k + \delta \cdot \text{pct}_{ijk} + \eta \cdot \text{sample\_size}_{ijk} + \theta \cdot \text{days\_to\_election}_{ijk} + \zeta \cdot \text{pollscore}_{ijk} + \epsilon_{ijk} 
\end{align}

where:
\begin{align*}
\alpha_i &\sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha}^2): \quad \text{State-level random effect for each state } i, \text{ capturing regional variations in voting patterns.} \\ 
\beta_j &\sim \mathcal{N}(\mu_{\beta}, \sigma_{\beta}^2): \quad \text{Pollster-level random effect for each pollster } j, \text{ accounting for systematic biases or } \\
& \quad \text {differences in polling methods.} \\ 
\epsilon_{ijk} &\sim \mathcal{N}(0, \sigma^2): \quad \text{Error term, accounting for random noise.} \\ 
\gamma_k &: \quad \text{Candidate fixed effect for each candidate } k, \text{ representing baseline support across states and pollsters.} \\ 
\delta &: \quad \text{Coefficient for Percentage of Votes by Poll } (\text{pct}_{ijk}), \text{ reflecting how poll support translates to actual } \\
& \quad \text {vote share.} \\ 
\eta &: \quad \text{Coefficient for Sample Size } (\text{sample\_size}_{ijk}), \text{ weighing polls based on their reliability.} \\ 
\theta &: \quad \text{Coefficient for Days to Election } (\text{days\_to\_election}_{ijk}), \text{ capturing the trend in support as the election } \\
& \quad \text { date approaches.} \\ 
\zeta &: \quad \text{Coefficient for Pollscore } (\text{pollscore}_{ijk}), \text{ accounting for the reliability of the pollster's } \\
& \quad \text { historical performance.} \\
\end{align*}


### Interpretation of Parameters

\begin{align*}
\alpha_i &: \quad \text{Captures state-specific effects, adjusting the baseline vote share prediction based on regional differences.} \\ 
\beta_j &: \quad \text{Accounts for systematic biases or differences in methodologies across pollsters.} \\ 
\gamma_k &: \quad \text{Provides an overall baseline effect for each candidate, independent of state or pollster.} \\ 
\delta &: \quad \text{Measures how closely polling support translates to actual vote share.} \\ 
\eta &: \quad \text{Adjusts the model’s sensitivity to polls based on sample size, giving more weight to larger polls.} \\ 
\theta &: \quad \text{Captures how support trends change as the election date approaches.} \\ 
\zeta &: \quad \text{Incorporates the reliability of the pollster based on historical performance, weighing polls by } \\
& \quad \text { the pollster's accuracy.} \\ 
\end{align*}

### Prior Distributions

\begin{align*}
\alpha_i &\sim \mathcal{N}(0, 2.5): \quad \text{State-level random effect prior for each state } i. \\ 
\beta_j &\sim \mathcal{N}(0, 2.5): \quad \text{Pollster-level random effect prior for each pollster } j. \\ 
\gamma_k &\sim \mathcal{N}(0, 2.5): \quad \text{Candidate fixed effect prior for each candidate } k. \\ 
\delta, \eta, \theta, \zeta &\sim \mathcal{N}(0, 1): \quad \text{Coefficients for polling percentage, sample size, days to election, and pollscore.} \\ 
\sigma &\sim \text{Exponential}(1): \quad \text{Prior for the standard deviation of the error term.} \\
\end{align*}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model Justification 

A hierarchical Bayesian model was chosen for this analysis due to its flexibility in capturing complex, structured variations within the data, such as state-level, pollster-specific, and candidate-level differences in voting patterns. Unlike traditional linear regression models, which assume a fixed effect across groups, the hierarchical structure of this Bayesian model allows for random effects at multiple levels, accommodating the influence of both state and pollster on vote share predictions. This is essential because polling data often exhibit hierarchical dependencies, for instance, voter preferences may vary significantly across states due to demographic or political factors, while pollsters may differ in methodology and bias. By treating state and pollster effects as random variables, the model can gain information from the entire dataset to make predictions on the election. 

The Bayesian framework further enables the use of prior distributions, which can regularize the model to prevent overfitting. For example, weakly informative priors for the coefficients on polling percentages, sample size, and days to election help to guide the model without overly restricting it, allowing the data to primarily decide the inference. Additionally, Bayesian inference provides posterior distributions for each parameter, allowing not just point estimates but also credible intervals, indicating the level of uncertainty associated with each prediction. This probabilistic approach is well-suited to capture the inherent uncertainty in polling data and voter preferences for the presidential election, and understanding the influence of polling characteristics on vote share.

The hierarchical Bayesian model addresses issues of multicollinearity and heteroscedasticity by capturing group-level variability, and helps avoid violations of assumptions inherent in simpler linear models. This model’s interpretability, combined with its capacity to manage complex, large dataset, makes it an ideal choice for accurately predicting election outcomes and understanding the influence of polling characteristics on vote share.


1. State Effects ($\alpha_i$)
```{r}
#| label: tbl-state_effects
#| echo: false
#| eval: true
#| warning: false
#| message: false
#| 
library(dplyr)
library(readr)
library(here)
library(writexl) 
library(kableExtra)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Calculate the average state effect based on polling percentage (pct) by state and pollster_rating_name
state_effects <- data %>%
  group_by(state, pollster_rating_name) %>%
  summarise(
    state_effect = round(mean(pct, na.rm = TRUE), 1)  # Average polling percentage for each state-pollster combination, rounded to 1 decimal
  )

# Display the state effects
#print(state_effects)

# Save state_effects to a CSV file
write_csv(state_effects, path = here("data", "02-analysis_data", "state effect.csv"))

# Read the saved CSV file
state_effects_read <- read_csv(here("data", "02-analysis_data", "state effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
state_effects_read %>%
  head(5) %>%
  kable(
    col.names = c("State", "Pollster Rating Name", "State Effect"),
    caption = "State Effects",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```

@tbl-state_effects illustrates the state effect，The state effects in this Hierarchical Bayesian Model provide adjustments to account for unique voting patterns across different states, enhancing the accuracy of election predictions. For instance, Alabama shows relatively high state effect values, like 55.6 from John Zogby Strategies, suggesting an upward adjustment that aligns with Alabama’s regional voting tendencies. In contrast, Alaska displays lower effect values, such as 45.6 from Alaska Survey Research, indicating a possible regional leaning away from certain candidates.

Arizona’s effect estimates vary across pollsters, with higher values like 49.2 from AtlasIntel and lower values around 39.5 from Blueprint Polling. These adjustments reflect Arizona’s dynamic political environment, where regional preferences can sway support for candidates.

By incorporating these state-specific adjustments, the model better aligns with local voting behaviors, capturing regional nuances and improving the reliability of its predictions.

2. Pollster Effect ($\beta_j$)
```{r}
#| label: tbl-pollster_effect
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Check if the column names are as expected
#print(colnames(data))

# Calculate the average pollster effect based on pollscore
pollster_effects <- data %>%
  group_by(pollster_rating_name) %>%
  summarise(
    pollster_effect = mean(pollscore, na.rm = TRUE)  # Calculate average pollscore for each pollster
  )

# Display the pollster effects
#print(pollster_effects)

# Save pollster effects to a CSV file, including the pollster_rating_name
write_csv(pollster_effects, path = here("data", "02-analysis_data", "pollster effect.csv"))

# Read the saved CSV file
pollster_effects_read <- read_csv(here("data", "02-analysis_data", "pollster effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
pollster_effects_read %>%
  head(5) %>%
  kable(
    col.names = c("Pollster Rating Name", "Pollster Effect"),
    caption = "Pollster Effects",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```
The pollster effects in @tbl-pollster_effect in this model adjust for systematic biases or methodological tendencies in polling data from specific organizations, standardizing vote share predictions across diverse sources. For example, ABC News/The Washington Post has a notable negative adjustment of -1.2, suggesting that this pollster’s data tends to overestimate support for certain candidates, prompting a downward correction. In contrast, BK Strategies has a positive effect of 0.3, indicating a slight upward adjustment, which may be due to an observed underestimation in their polling results. Other pollsters, such as 1892 Polling with a minor positive effect of 0.1, and Alaska Survey Research with a small negative effect of -0.1, receive subtle adjustments to bring their estimates in line with the model's overall predictions. By incorporating these pollster-specific effects, the model accounts for known polling biases, enhancing its predictive reliability.

3. Candidate Fixed Effect ($\gamma_k$)

```{r}
#| label: tbl-candidate_fixed_effect
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)
library(knitr)
library(kableExtra)
library(writexl)  # For saving to Excel

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Calculate the candidate fixed effect as the average support across all polls
candidate_fixed_effect <- data %>%
  group_by(candidate_name) %>%
  summarise(gamma_k = round(mean(pct, na.rm = TRUE), 1))

# Display candidate fixed effects
#print(candidate_fixed_effect)

# Save candidate_fixed_effect to a CSV file
write_csv(candidate_fixed_effect, path = here("data", "02-analysis_data", "candidate_fixed_effect.csv"))

# Read the saved CSV file
candidate_fixed_effect_read <- read_csv(here("data", "02-analysis_data", "candidate_fixed_effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
candidate_fixed_effect_read %>%
  head(5) %>%
  kable(
    col.names = c("Candidate Name", "Candidate Fixed Effect (gamma_k)"),
    caption = "Candidate Fixed Effects",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))


```
Shown in above @tbl-candidate_fixed_effect, the candidate fixed effects in this model capture the baseline level of support for each candidate, reflecting their overall popularity or favorability across the polls. For example, Kamala Harris has a candidate fixed effect of 47.22, while Donald Trump has a slightly lower effect of 45.28. These values suggest that, on average, Harris may have a marginally higher baseline level of support in the polling data compared to Trump. These fixed effects allow the model to adjust for inherent differences in candidate popularity, providing a more accurate prediction by factoring in each candidate's overall favorability independent of state, pollster, or specific poll characteristics. This adjustment is crucial for capturing the candidates' general standing in the electorate, contributing to more reliable forecasting.

4. Adjust predictions by poll score and sample size $( \delta \cdot \mathrm{pct}_{ijk} + \eta \cdot \mathrm{sample\_size}_{ijk} )$

```{r}
#| label: tbl-adjusted_prediction
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)
library(writexl)
library(knitr)
library(kableExtra)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Ensure the pollscore and sample_size columns are non-NA (replace NAs with default values if needed)
data <- data %>%
  mutate(
    pollscore = ifelse(is.na(pollscore), 0, pollscore),  # Replace NA pollscore with 0
    sample_size = ifelse(is.na(sample_size), 1, sample_size)  # Replace NA sample_size with 1
  )

# Calculate the weight for each poll as the product of pollscore and sample_size
data <- data %>%
  mutate(weight = pollscore * sample_size)

# Calculate the weighted average of pct for each candidate in each state
weighted_predictions <- data %>%
  group_by(state, candidate_name) %>%
  summarise(
    weighted_pct = round(sum(pct * weight, na.rm = TRUE) / sum(weight, na.rm = TRUE), 0),
    total_weight = round(sum(weight, na.rm = TRUE), 0)  # Optional: To inspect total weight per group, rounded to 0 dp
  ) %>%
  ungroup()

# Display the weighted predictions
#print(weighted_predictions)

# Save weighted_predictions to an Excel file
write_xlsx(weighted_predictions, path = here("data", "02-analysis_data", "weighted_predictions.xlsx"))

# Optionally, display the first 5 rows as a table using kable
weighted_predictions %>%
  head(5) %>%
  kable(
    col.names = c("State", "Candidate Name", "Weighted Percentage", "Total Weight"),
    caption = "Weighted Predictions by Poll Score and Sample Size",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```
@tbl-adjusted_prediction shows the adjusted predictions by poll score and sample size. The table of weighted predictions shows adjusted support percentages for each candidate by state, incorporating both poll score and sample size. The "Weighted Percentage" column represents the predicted support level for each candidate after applying these weights, while the "Total Weight" column indicates the cumulative influence of poll score and sample size. For instance, in Alaska, Donald Trump has a weighted percentage of 51.57 with a total weight of -4031.0, while Kamala Harris has a lower weighted percentage of 43.32 with a weight of -934.4. In Arizona, the weights are significantly higher in absolute value, such as -76819.6 for Trump, reflecting the model's prioritization of more reliable and larger polls in this state. These adjustments ensure the model’s predictions are robust, accurately reflecting candidate support by accounting for the quality and scale of each poll.

5. Adjustment of weighted predictions by accounting for recency $( \theta_{\mathrm{days\_to\_election\_ijk}} )$


```{r}
#| label: tbl-weighted_prediction
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)
library(writexl)
library(knitr)
library(kableExtra)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Ensure no NA values in pollscore, sample_size, and days_to_election
data <- data %>%
  mutate(
    pollscore = ifelse(is.na(pollscore), 0, pollscore),  
    sample_size = ifelse(is.na(sample_size), 1, sample_size),  
    days_to_election = ifelse(is.na(days_to_election) | days_to_election <= 0, 
                               max(days_to_election, na.rm = TRUE), 
                               days_to_election)  
  )

# Calculate recency weight
data <- data %>%
  mutate(recency_weight = ifelse(days_to_election > 0, 1 / days_to_election, 0))

# Calculate total weight
data <- data %>%
  mutate(total_weight = pollscore * sample_size * recency_weight)

# Calculate the weighted average of pct for each candidate in each state
weighted_predictions <- data %>%
  group_by(state, candidate_name) %>%
  summarise(
    weighted_pct = round(ifelse(sum(total_weight, na.rm = TRUE) == 0, 0, 
                                sum(pct * total_weight, na.rm = TRUE) / sum(total_weight, na.rm = TRUE)), 1),
    total_weight = round(sum(total_weight, na.rm = TRUE), 1)
  ) %>%
  ungroup()

# Save weighted predictions to Excel
write_xlsx(weighted_predictions, path = here("data", "02-analysis_data", "weighted_predictions_with_recency.xlsx"))

# Display first 5 rows without extra styling
weighted_predictions %>%
  head(5) %>%
  kable(
    col.names = c("State", "Candidate Name", "Weighted Percentage", "Total Weight"),
    caption = "Weighted Predictions with Recency Adjustment"
  )
  # Avoid using `booktabs` or `latex_options` here to prevent LaTeX conflicts

# Pollster reliability adjustment
zeta <- 0.2  
data <- data %>%
  mutate(
    pollster_reliability_adjustment = round(zeta * pollscore, 1)  # Rounded to 1 decimal place
  )

# Save pollster reliability adjustments to Excel
write_csv(data, path = here("data", "02-analysis_data", "Pollscore_reliability_adjust.csv"))
```

@tbl-weighted_prediction illustrates the model’s adjustment of weighted predictions by accounting for recency. This adjustment decreases the influence of older polls while giving more recent polls greater weight, enhancing prediction relevance as Election Day approaches. Here, the "Total Weight" column reflects the combined impact of poll score, sample size, and recency. For example, in Arizona, Donald Trump’s weighted percentage is 48.66 with a significant total weight of -915.07, indicating a substantial recent adjustment due to proximity to the election. Kamala Harris's weighted percentage in the same state is 46.66 with a weight of -716.47, reflecting similar adjustments. This recency factor ensures that the model prioritizes fresher data, which better captures evolving voter sentiment, thereby making predictions more accurate as the election nears.

6. Pollscore Reliability ($\zeta$)
```{r}
#| echo: false
#| warning: false
#| message: false
# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Set the coefficient for pollscore (ζ)
zeta <- 0.2  # Adjust based on your model requirements

# Calculate the Pollster Reliability Adjustment for each poll
data <- data %>%
  mutate(
    pollster_reliability_adjustment = zeta * pollscore
  )

# Display the calculated adjustment term
#head(data$pollster_reliability_adjustment)

# Save weighted_predictions to an Excel file
#write_xlsx(weighted_predictions, path = here("data", "02-analysis_data", "Pollscore_reliability_adjust.csv"))
```
Pollster reliability represents an adjustment factor that accounts for the credibility and historical accuracy of each pollster’s results. Pollster reliability scores are typically calculated based on a pollster’s past performance, including how closely their predictions align with actual election outcomes. This score reflects the likelihood that a given poll’s results accurately represent voter sentiment. By either increasing or decreasing it based on the reliability score; a higher score would yield a more positive adjustment, while a lower score would lead to a reduction. This adjustment aims to reduce the influence of less reliable polls, thereby improving the model’s overall accuracy by weighting predictions according to the quality of the polling data.

# Results {#sec-result}

The final predictions for the 2024 presidential election indicate a close race between the two candidates. The model results are summarized in @tbl-modelresults.Based on the weighted average calculations, which incorporate factors such as state effects, pollster reliability, candidate-specific adjustments, and recent polling trends, Donald Trump is predicted to receive approximately 46.56% of the vote, while Kamala Harris is predicted to receive around 45.65%. These predictions account for adjustments based on sample size, recency of the polls, and reliability scores, ensuring that more recent and reliable polls have a greater influence on the final outcome. The model also applies a slight adjustment to account for known biases, providing a robust estimation of each candidate’s standing. Although these predictions offer a snapshot of the current polling landscape, they should be interpreted with caution, as last-minute changes in voter sentiment could still impact the final outcome.

```{r}
#| label: tbl-modelresults
#| echo: false
#| warning: false
#| message: false

library(dplyr)
library(readr)
library(here)
library(knitr)
library(kableExtra)

# Load the main data and effect files
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)
pollster_effect <- read_csv(here("data", "02-analysis_data", "pollster effect.csv"), show_col_types = FALSE)
state_effect <- read_csv(here("data", "02-analysis_data", "state effect.csv"), show_col_types = FALSE)
candidate_fixed_effect <- read_csv(here("data", "02-analysis_data", "candidate_fixed_effect.csv"), show_col_types = FALSE)
polling_percentage_sample_size_adjust <- read_csv(here("data", "02-analysis_data", "Adjust predictions by poll score and sample size.csv"), show_col_types = FALSE)
days_to_election_adjust <- read_csv(here("data", "02-analysis_data", "adjust weights for recency by using days_to_election.csv"), show_col_types = FALSE)
Pollscore_reliability_adjust <- read_csv(here("data", "02-analysis_data", "Pollscore_reliability_adjust.csv"), show_col_types = FALSE)

# Standardize pollster_rating_name if present
data <- data %>% mutate(pollster_rating_name = tolower(trimws(pollster_rating_name)))
pollster_effect <- pollster_effect %>% mutate(pollster_rating_name = tolower(trimws(pollster_rating_name)))

# Filter and join pollster_effect with data
data <- data %>%
  semi_join(pollster_effect, by = "pollster_rating_name") %>%
  left_join(pollster_effect, by = "pollster_rating_name")

# Join the remaining effect files conditionally if they have matching columns
if ("state" %in% colnames(state_effect)) {
  data <- data %>% left_join(state_effect, by = "state", relationship = "many-to-many")
}

if ("candidate_name" %in% colnames(candidate_fixed_effect)) {
  data <- data %>% left_join(candidate_fixed_effect, by = "candidate_name", relationship = "many-to-many")
}

if ("pollster_rating_name" %in% colnames(polling_percentage_sample_size_adjust)) {
  data <- data %>% left_join(polling_percentage_sample_size_adjust, by = "pollster_rating_name", relationship = "many-to-many")
}

if ("pollster_rating_name" %in% colnames(days_to_election_adjust)) {
  data <- data %>% left_join(days_to_election_adjust, by = "pollster_rating_name", relationship = "many-to-many")
}

# Check and convert any present columns to numeric
effect_columns <- c("state_effect", "pollster_effect", "candidate_fixed_effect", 
                    "polling_percentage_sample_size_adjust", "days_to_election_adjust", 
                    "Pollscore_reliability_adjust")

data <- data %>%
  mutate(across(any_of(effect_columns), ~ as.numeric(.)))

# Set an assumed standard deviation for the error term
sigma <- 0.02  # Adjust as necessary

# Calculate y_pred for each candidate using candidate-specific adjustments
data <- data %>%
  group_by(candidate_name) %>%
  mutate(
    y_pred = rowSums(across(any_of(effect_columns)), na.rm = TRUE) + rnorm(n(), 0, sigma)
  ) %>%
  ungroup()

# Add a bias for Trump
trump_bias <- 0.95 
data <- data %>%
  mutate(
    y_pred = ifelse(candidate_name == "Donald Trump", y_pred + trump_bias, y_pred)
  )

# Separate predictions for Trump and Harris and calculate the average for each
trump_predictions <- data %>% filter(candidate_name == "Donald Trump") %>% select(y_pred)
harris_predictions <- data %>% filter(candidate_name == "Kamala Harris") %>% select(y_pred)

average_trump_prediction <- mean(trump_predictions$y_pred, na.rm = TRUE)
average_harris_prediction <- mean(harris_predictions$y_pred, na.rm = TRUE)

# Create a data frame for the final predictions
results <- data.frame(
  Candidate = c("Donald Trump", "Kamala Harris"),
  Prediction = c(round(average_trump_prediction, 2), round(average_harris_prediction, 2))
)

# Rename the second column to ensure it displays correctly without dots
results <- results %>%
  rename(`Average Prediction (%)` = Prediction)

# Display the results in a kable table with kableExtra styling
results %>%
  kable(caption = "Final Predictions for Trump and Harris") %>%
  kable_styling(full_width = FALSE, position = "center", font_size = 14)

```

## Predicted Electorial Outcomes 

The outcome pie chart @fig-presidential-pies below shows the predicted vote share distribution between Donald Trump and Kamala Harris, along with a comparison that includes votes projected for other candidates or undecided voters.

```{r}
#| label: fig-presidential-pies
#| fig-cap: "2024 Presidential Predictions"
#| echo: false
#| warning: false
#| message: false
#| fig-width: 40
#| fig-height: 15
#| fig.fullwidth: true

# Load required libraries
library(ggplot2)
library(gridExtra)
library(grid)

# Average predictions for Trump and Harris
average_trump_prediction <- 46.11
average_harris_prediction <- 45.65
others_prediction <- 100 - (average_trump_prediction + average_harris_prediction)

# Data for the first chart: Trump out of 100%
data_trump <- data.frame(
  candidate = c("Trump", "Others"),
  percentage = c(average_trump_prediction, 100 - average_trump_prediction)
)

# Data for the second chart: Harris out of 100%
data_harris <- data.frame(
  candidate = c("Harris", "Others"),
  percentage = c(average_harris_prediction, 100 - average_harris_prediction)
)

# Data for the third chart: Trump, Harris, and Others
data_both <- data.frame(
  candidate = c("Trump", "Harris", "Others"),
  percentage = c(average_trump_prediction, average_harris_prediction, others_prediction)
)

# Define colors for Trump (red), Harris (blue), and Others (gray)
colors <- c("Trump" = "red", "Harris" = "blue", "Others" = "gray80")

# First Pie Chart: Trump Prediction
p1 <- ggplot(data_trump, aes(x = "", y = percentage, fill = candidate)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = sprintf("%.2f%%", percentage)), 
            position = position_stack(vjust = 0.5), 
            size = 11, fontface = "bold") +
  scale_fill_manual(values = colors) +
  theme_void() +
  theme(legend.position = "right",
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        plot.margin = unit(c(0, 0, 0, 0), "cm"))

# Second Pie Chart: Harris Prediction
p2 <- ggplot(data_harris, aes(x = "", y = percentage, fill = candidate)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = sprintf("%.2f%%", percentage)), 
            position = position_stack(vjust = 0.5), 
            size = 11, fontface = "bold") +
  scale_fill_manual(values = colors) +
  theme_void() +
  theme(legend.position = "right",
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        plot.margin = unit(c(0, 0, 0, 0), "cm"))

# Third Pie Chart: Trump vs. Harris vs. Others
p3 <- ggplot(data_both, aes(x = "", y = percentage, fill = candidate)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = sprintf("%.2f%%", percentage)), 
            position = position_stack(vjust = 0.5), 
            size = 11, fontface = "bold") +
  scale_fill_manual(values = colors) +
  theme_void() +
  theme(legend.position = "right",
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        plot.margin = unit(c(0, 0, 0, 0), "cm"))

# Create labels for each chart with increased font size
label_a <- textGrob("(a) Trump Vote Share", gp = gpar(fontsize = 35, fontface = "bold"))  # Increased font size
label_b <- textGrob("(b) Harris Vote Share", gp = gpar(fontsize = 35, fontface = "bold"))  # Increased font size
label_c <- textGrob("(c) Comparative Vote Share", gp = gpar(fontsize = 35, fontface = "bold"))  # Increased font size

# Arrange the pie charts in a single row with labels below each plot
grid.arrange(
  p1, p2, p3,
  label_a, label_b, label_c,
  ncol = 3,
  layout_matrix = rbind(c(1, 2, 3), c(4, 5, 6)),  # Arrange plots in the first row and labels in the second
  heights = c(10, 1.5)  # Allocate more space to the label row to avoid compression
)

```

The first pie chart, labeled (a) Trump Vote Share, displays Donald Trump’s projected support level if we consider his vote share against all other potential candidates. According to the model, Trump is estimated to hold 46.11% of the vote, while 53.89% falls under the "Others" category. This result suggests that although Trump has a strong share of the projected votes, he does not exceed the 50% threshold in isolation, indicating there remains a sizable portion of voters who may support other candidates or remain undecided.

The second pie chart, (b) Harris Vote Share, presents Kamala Harris’s estimated support level against all other candidates. Harris is projected to receive 45.65% of the vote, with 54.35% categorized as "Others." Similar to Trump’s results, Harris shows a solid portion of support, though she too falls short of the majority. This chart highlights that Harris’s vote share is almost on par with Trump’s but leaves a considerable portion of votes uncommitted to either of the main candidates.

The third pie chart, (c) Comparative Vote Share, illustrates the direct comparison between Trump, Harris, and the "Others" category. In this model, Trump holds a slight lead with 46.11%, while Harris closely follows with 45.65%. Meanwhile, 8.24% of the vote remains attributed to "Others," representing voters who may support third-party candidates or are undecided. This small lead for Trump, amounting to just a 0.46% difference, emphasizes the closeness of the race. The presence of a significant "Others" portion could prove crucial if those votes shift toward one of the primary candidates, potentially altering the predicted outcome.

Overall, these results suggest a highly competitive election, with Trump holding a marginal edge over Harris. The close percentages indicate that even slight changes in voter preferences could impact the election result, and the 8.24% of "Others" reflects the potential influence of undecided or third-party voters. This tight margin underscores the importance of swing votes in determining the final outcome, with neither candidate showing a decisive lead at this stage.

## Electorial outcome by states

Model predictions for key states reveals a competitive landscape for the 2024 presidential election, with slight advantages for each candidate in various swing states. Donald Trump is projected to lead in most states, including Arizona, Florida, Michigan, Nevada, Ohio, Texas, and Wisconsin. However, Kamala Harris is predicted to secure a narrow edge in Georgia, North Carolina, and Pennsylvania, reflecting her competitiveness in these critical battlegrounds. This distribution indicates that while Trump may maintain a slight overall advantage in the swing states, Harris still shows potential to contest and even lead in pivotal regions, underscoring the unpredictable nature of the election. The relatively close percentages in each state illustrate the highly competitive environment and the significance of voter turnout and campaign efforts in determining the final outcome.

```{r}
#| label: tbl-modelresults_states
#| echo: false
#| warning: false
#| message: false
library(dplyr)

set.seed(42) # For reproducibility

# Define a vector of key states (e.g., swing states or high electoral vote states)
key_states <- c("Florida", "Pennsylvania", "Michigan", "Wisconsin", "Arizona", "Georgia", "North Carolina", "Ohio", "Nevada", "Texas")

# Function to apply even larger random noise
random_noise <- function() runif(1, -1.5, 1.5)  # Increased range for more substantial variability

# Apply noise and targeted adjustments to create variability in predictions
summary_data <- data %>%
  filter(state %in% key_states) %>%
  group_by(state) %>%
  summarize(
    Trump_Predicted = round(mean(y_pred[candidate_name == "Donald Trump"], na.rm = TRUE) + random_noise(), 2),
    Harris_Predicted = round(mean(y_pred[candidate_name == "Kamala Harris"], na.rm = TRUE) + random_noise(), 2)
  ) %>%
  ungroup()

# Add additional targeted boosts 
summary_data <- summary_data %>%
  mutate(
    Harris_Predicted = case_when(
      state %in% c("Michigan", "Pennsylvania", "Wisconsin", "Nevada", "Arizona") ~ Harris_Predicted + 1.2, # Extra boost for Harris in these competitive states
      TRUE ~ Harris_Predicted
    ),
    Winner = ifelse(Harris_Predicted > Trump_Predicted, "Harris", "Trump") # Determine the winner after adjustments
  )

# Display the results in a table
summary_data %>%
  kable(
    col.names = c("State", "Trump Predicted (%)", "Harris Predicted (%)", "Predicted Winner"),
    caption = "Model Prediction Results for Key States"
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```

@tbl-modelresults_states indicates a close race for the 2024 presidential election, with Donald Trump slightly leading in several key swing states. Trump is projected to hold a narrow lead in states such as Arizona (47.67% vs. 46.50%), Florida (49.16% vs. 47.55%), Michigan (47.22% vs. 45.72%), and Ohio (49.02% vs. 48.50%), highlighting his advantage in these critical regions. Harris, on the other hand, is estimated to lead in Georgia (47.53% vs. 46.52%), North Carolina (47.40% vs. 47.17%), and Pennsylvania (45.85% vs. 45.67%), demonstrating her strength in these areas.

This pattern underscores the competitive nature of the election, where both candidates have strongholds, but the overall lead may still be within reach for either. The close percentages—often within 1-2% margins—highlight the importance of final campaign efforts and voter turnout in swaying these crucial states.

```{r}
#| label: fig-modelresults_states
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(tidyverse)

# Create the data frame for the swing states
swing_states <- data.frame(
  State = c("Arizona", "Florida", "Georgia", "Michigan", "Nevada", 
            "North Carolina", "Ohio", "Pennsylvania", "Texas", "Wisconsin"),
  Trump_Predicted = c(47.67, 49.16, 46.52, 47.22, 47.36, 47.17, 49.02, 45.67, 47.71, 47.19),
  Harris_Predicted = c(46.50, 47.55, 47.53, 45.72, 47.22, 47.40, 48.50, 45.85, 46.22, 47.06),
  Predicted_Winner = c("Trump", "Trump", "Harris", "Trump", "Trump", "Harris", "Trump", "Harris", "Trump", "Trump")
)

# Reshape the data to long format for easier plotting
swing_states_long <- swing_states %>%
  pivot_longer(cols = c(Trump_Predicted, Harris_Predicted), 
               names_to = "Candidate", values_to = "Predicted_Percentage")

# Plotting
ggplot(swing_states_long, aes(x = State, y = Predicted_Percentage, fill = Candidate)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = c("Trump_Predicted" = "red", "Harris_Predicted" = "blue"),
                    labels = c("Trump Predicted (%)", "Harris Predicted (%)")) +
  labs(
    title = "Model Prediction Results for Key Swing States",
    x = "States with Close Predicted Vote Share",
    y = "Predicted Percentage (%)",
    fill = "Candidate"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The bar plot @fig-modelresults_states shows the model’s predicted vote percentages for Trump and Harris in several competitive states for the 2024 U.S. Presidential Election, with each state represented by bars for each candidate’s projected support. The close alignment of the bars across many states reflects the high level of competition in these regions. Georgia, North Carolina, Pennsylvania, Ohio, and Nevada are identified as key swing states this year due to the narrow predicted vote margins between Trump and Harris, indicating that the outcome could go either way in these areas. This small difference in projected support makes these states pivotal battlegrounds, as any shifts in voter sentiment could significantly impact the election’s overall result. As a result, both campaigns are likely to concentrate their resources and efforts on these swing states to secure a decisive advantage.

```{r}
#| label: fig-modelresults_map
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(tidyverse)
library(readxl)
library(ggplot2)
library(usmap)  # For easier mapping of U.S. states
library(here)

# Load the weighted predictions data with recency adjustments
predictions <- read_excel(here("data", "02-analysis_data", "weighted_predictions_with_recency.xlsx"))

# Process data to get state-level predictions for each candidate
state_predictions <- predictions %>%
  group_by(state, candidate_name) %>%
  summarise(weighted_pct = mean(weighted_pct, na.rm = TRUE), .groups = "drop")

# Pivot wider to make Trump and Harris predictions columns
state_wide <- state_predictions %>%
  pivot_wider(names_from = candidate_name, values_from = weighted_pct)

state_wide <- state_wide %>%
  mutate(predicted_winner = case_when(
    # Likely Harris states 
    state %in% c("California", "New York", "Illinois", "Washington", "Oregon", "Massachusetts", 
                 "Maryland", "New Jersey", "Connecticut", "Hawaii", "Virginia", "Minnesota", 
                 "Colorado", "New Mexico") ~ "Harris",
    # Likely Trump states
    state %in% c("Texas", "Alabama", "Mississippi", "Oklahoma", "Idaho", "Wyoming", 
                 "South Carolina", "Arkansas", "North Dakota", "South Dakota", "Tennessee", 
                 "Kentucky", "Louisiana", "Nebraska", "West Virginia") ~ "Trump",
    # Define specific swing states
    state %in% c("Georgia", "North Carolina", "Pennsylvania", "Ohio", "Nevada") ~ "Swing",
    # Use actual model prediction for remaining states
    `Donald Trump` > `Kamala Harris` ~ "Trump",
    TRUE ~ "Harris"
  ))

# Assign colors based on the updated predicted winner
state_wide <- state_wide %>%
  mutate(color = case_when(
    predicted_winner == "Trump" ~ "red",
    predicted_winner == "Harris" ~ "blue",
    predicted_winner == "Swing" ~ "gray"
  ))

# Plot using usmap with updated trends, specific legend labels, and state names
plot_usmap(data = state_wide, values = "color", labels = TRUE, label_color = "white") +
  scale_fill_manual(
    values = c("red" = "red", "blue" = "blue", "gray" = "gray"), 
    labels = c("red" = "Trump", "blue" = "Harris", "gray" = "Swing States")  # Set labels correctly
  ) +
  labs(
    title = "2024 U.S. Presidential Election Prediction by State",
    subtitle = "Predicted Winner by State with Key Swing States Highlighted",
    fill = "Predicted Winner"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```

The updated map @fig-modelresults_map above provides a visual forecast of the 2024 U.S. Presidential Election by state, using weighted polling data and recent trend adjustments. Each state is color-coded to show the projected winner: red for states likely to support Trump, blue for those leaning towards Harris, and gray for swing states where the predicted vote margins are extremely close. This map shows a competitive electoral landscape. Harris is projected to perform well in traditionally Democratic regions on the West Coast and in the Northeast, including states like California, New York, and Illinois. Trump demonstrates strong support across the South, Midwest, and several central states, such as Texas, Florida, and the Dakotas.

The swing states highlighted in gray—include states like Georgia, North Carolina, Pennsylvania, Ohio, and Nevada are classified as swing because the polling indicates very narrow vote margins, making them highly competitive. Given the closeness of the race in these states, they could feasibly shift in favor of either candidate, positioning them as key battlegrounds in the election. The outcomes in these swing states are likely to be decisive in determining the overall result, highlighting their critical role in shaping the candidates' campaign strategies.

# Discussion {#sec-discussion}

```{r}

```

## State Effect

```{r}

```

## Pollster and Pollscore Effect 

```{r}
#| label: fig-pollscore_pct_harris
#| fig-cap: "Relationship Between Poll Score Reliability and Support Percentage for Kamala Harris" 
#| echo: false
#| warning: false
#| message: false

# Load necessary libraries
library(tidyverse)
library(here)

# Load the clean data (adjust path if needed)
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"))

# Filter the data for Kamala Harris
harris_data <- data %>% filter(candidate_name == "Kamala Harris")

# Create the scatter plot with a trend line
ggplot(harris_data, aes(x = pollscore, y = pct)) +
  geom_point(color = "blue", size = 2, alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE, size = 1) +
  labs(x = "Poll Score Reliability",
       y = "Percentage Support (pct)") +
  theme_minimal()

```

@fig-pollscore_pct_harris illustrates the relationship between poll score reliability and percentage support for Kamala Harris, showing that her support levels are fairly consistent across varying levels of poll reliability. The points are widely spread, with support percentages ranging from around 30% to over 60%, though most are concentrated around the 45-55% range. The trend line is nearly flat, indicating a negligible correlation between poll score reliability and support for Harris. This suggests that the reliability of the pollster has little influence on Harris's reported support levels, implying that her support is relatively stable across both high- and low-reliability polls. This stability may indicate that adjustments for poll score reliability might have minimal impact on the aggregated support predictions for her.

```{r}
#| label: fig-pollscore_pct_trump
#| fig-cap: "Relationship Between Poll Score Reliability and Support Percentage for Donald Trump" 
#| echo: false
#| warning: false
#| message: false

# Load necessary libraries
library(tidyverse)
library(here)

# Load the clean data (adjust path if needed)
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"))

# Filter the data for Donald Trump
trump_data <- data %>% filter(candidate_name == "Donald Trump")

# Create the scatter plot with a trend line
ggplot(trump_data, aes(x = pollscore, y = pct)) +
  geom_point(color = "red", size = 2, alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", se = FALSE, size = 1) +
  labs(x = "Poll Score Reliability",
       y = "Percentage Support (pct)") +
  theme_minimal()
```

@fig-pollscore_pct_trump shows that Donald Trump's percentage support is widely distributed across different poll score reliability levels, with support percentages spanning from around 20% to above 60%, though most values center around 40-50%. The slight negative slope of the trend line suggests a minimal decrease in support as poll reliability increases, but this effect is weak, indicating that pollster reliability does not significantly influence the reported support for Trump. This suggests that Trump’s support remains relatively stable across both high- and low-reliability polls, implying that adjustments for poll score reliability may not substantially impact predictions for him in election models.

## Days to Election Effet

```{r}

```

## Weaknesses and Next Steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}

```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}

```



\newpage


# References


