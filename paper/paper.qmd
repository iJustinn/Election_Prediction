---
title: "Red, Blue, or Purple? 2024’s Battleground States Tell a Story"
subtitle: "Pennsylvania, Arizona, and Nevada Become Game Changers as Trump Gains Ground [To be Update]"
author: 
  - Yingke He
  - Ziheng Zhong
thanks: "Code and data are available at: [https://github.com/iJustinn/Election_Prediction](https://github.com/RohanAlexander/starter_folder)."
date: today
date-format: long
abstract: "This paper analyzes polling data for the 2024 U.S. Presidential election, examining the key candidates notabily Donald Trump and Kamala Harris to gain a comprehensive perspective on the race. A Hierarchical Bayesian Model is applied in this study and the findings reveal significant variations across states, with [To be updated...] emerging as particularly influential battlegrounds that ultimately tilt in favor of [To be updated..].By highlighting these trends, this research underscores the dynamic and evolving nature of U.S. political landscapes and the crucial role that swing states play in determining election outcomes."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(kableExtra)
library(ggplot2)
library(dplyr)
library(here)
library(knitr)
library(rstanarm)
library(modelsummary)

#read in cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

cleaned_poll_data <-
  read_csv(file = here("data", "02-analysis_data", "num_grade_pollscore_data.csv"), 
           show_col_types = FALSE)

#model <- stan_glmer(
#  pct ~ (1 | state) + (1 | pollster_rating_name) + candidate_name + sample_size + days_to_election,
#  data = data,
#  family = gaussian(), 
#  prior = normal(0, 2.5),
#  prior_intercept = normal(0, 2.5),
#  prior_aux = exponential(1),
#  iter = 2000, chains = 4
# )
```



# Introduction {#sec-intro}

In the lead-up to the 2024 U.S. Presidential election, understanding voter behavior across states and demographic groups is essential for accurately predicting electoral outcomes. Recent polling data from various sources suggests a close race among major candidates, including Donald Trump, Kamala Harris, and other key contenders. This study applies a Hierarchical Bayesian Model to analyze polling data across all U.S. states, with a focus on identifying critical swing states where even slight shifts in voter sentiment could prove decisive. By exploring trends in voter support across different states and demographic groups, this research aims to highlight the regions most likely to influence the final election outcome.

The primary objective of this study is to estimate state-level vote shares for each candidate while accounting for factors such as regional variations, pollster-specific biases, and the timing of polls relative to Election Day. Predictors including polling percentages, sample sizes, and days remaining until the election are incorporated into the model, which adjusts for state and pollster effects. The model’s structure enables the estimation of vote shares that reflect the underlying distribution of polling data across diverse regions in the U.S.

Initial findings reveal variations in candidate support across states, with certain swing states emerging as pivotal in determining the Electoral College result. The model’s projections suggest a likely lead for Trump in the Electoral College, underscoring the influence of large-sample, recent polls on forecast accuracy. This study’s results emphasize the importance of swing states in the electoral process, demonstrating how regional dynamics and polling methodologies impact predictions.[To Be updated...]

This research contributes to the field of election forecasting by combining aggregated polling data with a robust modeling approach, offering valuable insights for political analysts, campaign strategists, and policymakers. By identifying critical swing states and accounting for potential polling biases, this study equips stakeholders with the tools to anticipate voter shifts and strategize effectively in an evolving political landscape.

The structure of the paper is organized as follows: following @sec-intro, @sec-data outlines the data collection and cleaning process, along with a description of the outcome and predictor variables used in the analysis.@sec-model, introduces the forecasting models and discuss the rationale behind choosing these models for election outcomes prediction. @sec-result then presents the main findings, including a breakdown of state-level and pollster-level random effects. Finally, @sec-discussion interprets the results, highlighting significant trends and predictions, and concludes with a discussion on the reliability of the forecasts and potential limitations of the models.


# Data {#sec-data}

## Overview [info need update]

We use the statistical programming language R [@citeR].... Our data [@shelter].... Following @tellingstories, we consider...

Overview text [To be updated...]

## Data Measurement [info need update]
	
In this study, polling data provides insight into public opinion across different states, measuring support for candidates ahead of the 2024 U.S. Presidential election. The data originates from various reputable polling organizations, each employing distinct methodologies that influence data quality and representativeness. Understanding these methodologies is crucial for interpreting polling data accurately. The primary categories of measurement methods include:

1. Live Telephone Interviews
Some polling organizations, such as Selzer & Co. and Marist College, conduct live telephone interviews, reaching respondents on both landlines and cell phones. This method often provides more accurate and in-depth data because live interviewers can clarify questions, verify demographic details, and increase respondent engagement. However, it is costlier and time-intensive, leading to smaller sample sizes and potentially fewer surveys. In the real world, live telephone interviews offer insights from a cross-section of the population, though they may over-represent older individuals who are more likely to answer landline calls.

2. Interactive Voice Response (IVR)
Pollsters like Trafalgar Group and Rasmussen Reports frequently use IVR, or “robo-polling,” where an automated system asks questions over the phone. IVR is more affordable and efficient for reaching large numbers of people. However, because respondents interact with a machine rather than a live person, they may feel less invested in the survey, potentially impacting response rates and depth of engagement. IVR allows pollsters to obtain timely snapshots of voter intentions, though it may miss out on nuanced responses that a live interviewer could capture.

3. Online Panels
YouGov and Ipsos are known for relying on online panels, which recruit participants via web platforms to answer surveys electronically. This method allows for rapid data collection from a broad audience and is relatively cost-effective. However, online surveys can suffer from self-selection bias, as individuals who are more politically active or internet-savvy are more likely to participate. Despite this, online panels remain popular due to their scalability and cost-efficiency, providing a contemporary approach to measuring public sentiment.

4. Mixed-Mode Surveys (Telephone and Online)
To balance the benefits and limitations of each mode, organizations like The New York Times/Siena College and Public Policy Polling employ mixed-mode surveys. By combining online and telephone interviews, these pollsters aim to broaden demographic reach and mitigate biases inherent in any single method. Mixed-mode surveys improve representativeness and accommodate diverse respondent preferences, providing data that may better reflect the full spectrum of public opinion.

5. Specialized In-Person and University-Affiliated Research
Some university-affiliated pollsters, such as Quinnipiac University and University of New Hampshire Survey Center, incorporate face-to-face interviews or demographic-specific focus groups for deeper insights. These methods can yield highly detailed responses and foster a comprehensive understanding of specific voter groups, though they are resource-intensive and typically used for more specialized research rather than general polling.

These varied data collection methods directly affect how public opinion is measured and interpreted. For instance, live interviews may capture a more detailed view of voter sentiment but often exclude younger demographics who primarily use mobile devices or interact online. Meanwhile, IVR and online panels make large-scale data collection affordable and timely, though they may lack depth and introduce potential self-selection biases. Mixed-mode approaches provide a balanced perspective, catering to both traditional and digital respondents, which may yield more representative data.

The choice of methodology reflects each pollster’s objectives and resources, with implications for accuracy and data interpretation. By categorizing pollsters based on their methodologies, we can contextualize the polling data in this study, recognizing each method's strengths and limitations. This understanding is essential for modeling real-world voter behavior accurately, as it acknowledges how data collection methods impact polling results.

## Outcome variables


According to @tbl-outcome_variables, the outcome variables include critical components for analyzing polling data, such as State, Pollster, Pollscore, Candidate Name, Percentage (pct), Sample Size, Start Date, End Date, and Days to Election. These variables provide the foundation for estimating actual vote shares by capturing regional differences, pollster credibility, candidate-specific support, and timing in relation to Election Day. Together, they ensure a nuanced approach to election forecasting that accounts for variability in poll methodology and voter sentiment shifts over time.
```{r}
#| label: tbl-outcome_variables
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Define the variable names and examples
outcome_variables <- data.frame(
  Variable = c("State", "Pollster", "Pollscore","Candidate Name", "Percentage (pct)", "Sample Size", "Start Date", "End Date", "Days to Election"),
  Example = c("Wisconsin", "RMG Research","0.4" ,"Donald Trump", "48.5%", "789", "2024-9-24", "2024-10-16", "20")
)

# Create the table with an Example column
outcome_variables %>%
  kable(
    col.names = c("Outcome Variable", "Example"),
    caption = "Outcome Variables",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))
```
Above outcome variables @tbl-outcome_variables 


## Predictor variables 
This study includes several key predictor variables that provide insights into regional voting patterns, polling methodologies, and the timing of voter sentiment shifts. Each variable plays a unique role in forecasting vote shares for the 2024 U.S. Presidential election. Below shown in @tbl-predictor_variables are detailed descriptions of each predictor.

```{r}
#| label: tbl-predictor_variables
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Define the variable names
predictor_variables <- data.frame(
  Variable = c("State", "Pollster", "Pollscore","Candidate Name", "Percentage (pct)", "Sample Size", "Days to Election"),
  Example = c("Georgia", "InsiderAdvantage", "0.6","Kamala Harris", "47%", "800", "24")
)

# Create the table
predictor_variables %>%
  kable(
    col.names = c("Predictor Variable", "Example"),
    caption = "Predictor Variables",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))
```

### State

State (`state`) is a categorical variable that represents the U.S. state in which each poll was conducted. States have distinct political dynamics, influenced by regional issues, demographics, and historical voting patterns. Including the state variable allows the model to account for these differences, enabling predictions that are sensitive to each state’s unique electoral landscape.

### Pollster

Pollster (`pollster`) identifies the organization conducting each poll. Different polling organizations may use varied methodologies, such as sample selection, weighting, and question phrasing, which can introduce systematic biases. By including the pollster as a predictor, the model can adjust for potential biases or methodological differences, providing more accurate and standardized vote share estimates.

### Candidate Name 

Candidate Name (`candidate_name`) is a categorical variable that identifies the candidate for whom each poll measures support. Including candidate names allows the model to assess each candidate’s baseline popularity and identify variations in support across different demographics and states. This variable enables comparisons between major contenders, such as Donald Trump and Kamala Harris, as well as other candidates, to forecast overall vote shares.

### Percentage of Votes

Percentage of Votes (`pct`) represents the percentage of respondents in each poll who indicated support for a specific candidate. As a primary predictor, this variable directly informs the model about the current level of support for each candidate at the time the poll was conducted. By converting these percentages into vote share estimates, the model can project likely election outcomes based on current polling data.

### Sample Size

Sample Size (`sample_size`) denotes the number of respondents included in each poll. Larger sample sizes generally increase the reliability of a poll, as they reduce the margin of error and are more likely to represent the broader population. By incorporating sample size as a predictor, the model can weigh polls based on their reliability, giving more importance to larger, more robust samples.

### Pollscore

Pollscore (`pollscore`) variable serves as a measure of each pollster's historical accuracy or quality, reflecting how reliable or unbiased the pollster's results have been over time. A higher pollscore likely indicates a pollster with a better track record of accurate predictions, suggesting their polling methodologies or adjustments may be more robust.

### Days to Election
The variable days to election represents the days remaining from the poll's end date to Election Day.
It is calculated using the `end_date` variable, by subtracting the end_date from the election date, by assuming the election date is November 5 2024.[Add more...]

```{r}

```
days_to_election

# Model {#sec-model}

To predict the actual election vote share for each candidate in each state while accounting for variations by pollster and other poll-specific factors.
Background details and diagnostics are included in [Appendix -@sec-model-details]. [To be updated...]

## Model set-up

A Hierarchical Bayesian Model is utilized to predict the actual election vote for each candidate in each state while accounting for variables by pollster.

The model prediction utilizes the following predictor variables:

- **State** (`state`): Include as a categorical term to capture regional variations.
- **Pollster** (`pollster`): Include as a categorical variable for different polling effects.
- **Pollscore** (`pollscore`): A numerical variable reflecting how reliable the pollster's results have been over time.
- **Candidate Name** (`candidate_name`):Include as a categorical feature or model separately for each candidate.
- **Percentage of Votes by Poll in State** (`pct`): Use as a primary predictor of actual vote share, with a smooth term to allow for non-linear effects.
- **Sample Size** (`sample_size`): Include as a predictor or weight to reflect poll reliability.
- **Days to Election** (`end_date`):capture trends in support leading up to the election.


Let: 
\begin{align*}
y_{ijk} & : \text{ The target variable, representing the actual election vote share for candidate } k \text{ in state } \\
& \quad i \text{ with pollster } j. \\ 
\text{pct}_{ijk} & : \text{ The observed polling percentage for candidate } k \text{ in state } i \text{ by pollster } j. \\ 
\text{sample\_size}_{ijk} & : \text{ The sample size of the poll, which helps in weighing the poll reliability.} \\ 
\text{days\_to\_election}_{ijk} & : \text{ Derived as the days remaining until the election from the poll’s end date, capturing } \\
& \quad \text { the effect of recency.} \\ 
\text{pollscore}_{ijk} & : \text{ The reliability score for each pollster } j, \text{ reflecting the accuracy of the pollster's } \\
& \quad \text { historical results.} \\
\end{align*}

The model takes the form of the following equation:
\begin{align}
y_{ijk} = \alpha_i + \beta_j + \gamma_k + \delta \cdot \text{pct}_{ijk} + \eta \cdot \text{sample\_size}_{ijk} + \theta \cdot \text{days\_to\_election}_{ijk} + \zeta \cdot \text{pollscore}_{ijk} + \epsilon_{ijk} 
\end{align}

where:
\begin{align*}
\alpha_i &\sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha}^2): \quad \text{State-level random effect for each state } i, \text{ capturing regional variations in voting patterns.} \\ 
\beta_j &\sim \mathcal{N}(\mu_{\beta}, \sigma_{\beta}^2): \quad \text{Pollster-level random effect for each pollster } j, \text{ accounting for systematic biases or } \\
& \quad \text {differences in polling methods.} \\ 
\epsilon_{ijk} &\sim \mathcal{N}(0, \sigma^2): \quad \text{Error term, accounting for random noise.} \\ 
\gamma_k &: \quad \text{Candidate fixed effect for each candidate } k, \text{ representing baseline support across states and pollsters.} \\ 
\delta &: \quad \text{Coefficient for Percentage of Votes by Poll } (\text{pct}_{ijk}), \text{ reflecting how poll support translates to actual } \\
& \quad \text {vote share.} \\ 
\eta &: \quad \text{Coefficient for Sample Size } (\text{sample\_size}_{ijk}), \text{ weighing polls based on their reliability.} \\ 
\theta &: \quad \text{Coefficient for Days to Election } (\text{days\_to\_election}_{ijk}), \text{ capturing the trend in support as the election } \\
& \quad \text { date approaches.} \\ 
\zeta &: \quad \text{Coefficient for Pollscore } (\text{pollscore}_{ijk}), \text{ accounting for the reliability of the pollster's } \\
& \quad \text { historical performance.} \\
\end{align*}


### Interpretation of Parameters

\begin{align*}
\alpha_i &: \quad \text{Captures state-specific effects, adjusting the baseline vote share prediction based on regional differences.} \\ 
\beta_j &: \quad \text{Accounts for systematic biases or differences in methodologies across pollsters.} \\ 
\gamma_k &: \quad \text{Provides an overall baseline effect for each candidate, independent of state or pollster.} \\ 
\delta &: \quad \text{Measures how closely polling support translates to actual vote share.} \\ 
\eta &: \quad \text{Adjusts the model’s sensitivity to polls based on sample size, giving more weight to larger polls.} \\ 
\theta &: \quad \text{Captures how support trends change as the election date approaches.} \\ 
\zeta &: \quad \text{Incorporates the reliability of the pollster based on historical performance, weighing polls by } \\
& \quad \text { the pollster's accuracy.} \\ 
\end{align*}

### Prior Distributions

\begin{align*}
\alpha_i &\sim \mathcal{N}(0, 2.5): \quad \text{State-level random effect prior for each state } i. \\ 
\beta_j &\sim \mathcal{N}(0, 2.5): \quad \text{Pollster-level random effect prior for each pollster } j. \\ 
\gamma_k &\sim \mathcal{N}(0, 2.5): \quad \text{Candidate fixed effect prior for each candidate } k. \\ 
\delta, \eta, \theta, \zeta &\sim \mathcal{N}(0, 1): \quad \text{Coefficients for polling percentage, sample size, days to election, and pollscore.} \\ 
\sigma &\sim \text{Exponential}(1): \quad \text{Prior for the standard deviation of the error term.} \\
\end{align*}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model Justification 

A hierarchical Bayesian model was chosen for this analysis due to its flexibility in capturing complex, structured variations within the data, such as state-level, pollster-specific, and candidate-level differences in voting patterns. Unlike traditional linear regression models, which assume a fixed effect across groups, the hierarchical structure of this Bayesian model allows for random effects at multiple levels, accommodating the influence of both state and pollster on vote share predictions. This is essential because polling data often exhibit hierarchical dependencies, for instance, voter preferences may vary significantly across states due to demographic or political factors, while pollsters may differ in methodology and bias. By treating state and pollster effects as random variables, the model can gain information from the entire dataset to make predictions on the election. 

The Bayesian framework further enables the use of prior distributions, which can regularize the model to prevent overfitting. For example, weakly informative priors for the coefficients on polling percentages, sample size, and days to election help to guide the model without overly restricting it, allowing the data to primarily decide the inference. Additionally, Bayesian inference provides posterior distributions for each parameter, allowing not just point estimates but also credible intervals, indicating the level of uncertainty associated with each prediction. This probabilistic approach is well-suited to capture the inherent uncertainty in polling data and voter preferences for the presidential election, and understanding the influence of polling characteristics on vote share.

The hierarchical Bayesian model addresses issues of multicollinearity and heteroscedasticity by capturing group-level variability, and helps avoid violations of assumptions inherent in simpler linear models. This model’s interpretability, combined with its capacity to manage complex, large dataset, makes it an ideal choice for accurately predicting election outcomes and understanding the influence of polling characteristics on vote share.

1. Coefficient Estimates 
```{r}
#| label: tbl-coefficient_estimates
#| echo: false
#| warning: false
#| message: false

# Load necessary libraries
library(tidyverse)
library(kableExtra)

# Load the CSV file that contains only pollster effects
pollster_effect_data <- read_csv("pollster_effect_data.csv")

# Calculate the "Polling Score" column as the row-wise mean of all pollster columns
pollster_effect_data <- pollster_effect_data %>%
  mutate(polling_score = rowMeans(., na.rm = TRUE))

# Now we have a single column `polling_score` representing the general polling score effect per sample
# Calculate summary statistics for this "polling_score" column

polling_score_estimate <- mean(pollster_effect_data$polling_score, na.rm = TRUE)
polling_score_sd <- sd(pollster_effect_data$polling_score, na.rm = TRUE)
polling_score_ci <- quantile(pollster_effect_data$polling_score, probs = c(0.025, 0.975), na.rm = TRUE)

# Assuming we also have `sample_size` and `days_to_election` columns in the main dataset
# Load the main dataset if not already loaded
posterior_draws_df <- read_csv(here("data", "02-analysis_data", "fitted_model_posterior_draws.csv"), show_col_types = FALSE)

sample_size_estimate <- mean(posterior_draws_df$sample_size, na.rm = TRUE)
sample_size_sd <- sd(posterior_draws_df$sample_size, na.rm = TRUE)
sample_size_ci <- quantile(posterior_draws_df$sample_size, probs = c(0.025, 0.975), na.rm = TRUE)

days_to_election_estimate <- mean(posterior_draws_df$days_to_election, na.rm = TRUE)
days_to_election_sd <- sd(posterior_draws_df$days_to_election, na.rm = TRUE)
days_to_election_ci <- quantile(posterior_draws_df$days_to_election, probs = c(0.025, 0.975), na.rm = TRUE)

# Create a data frame for the summary table
coef_summary <- data.frame(
  Predictor = c("Sample Size", "Days to Election", "Polling Score"),
  Estimate = round(c(sample_size_estimate, days_to_election_estimate, polling_score_estimate), 4),
  `Std. Error` = round(c(sample_size_sd, days_to_election_sd, polling_score_sd), 4),
  `95% CI Lower` = round(c(sample_size_ci[1], days_to_election_ci[1], polling_score_ci[1]), 4),
  `95% CI Upper` = round(c(sample_size_ci[2], days_to_election_ci[2], polling_score_ci[2]), 4)
)

# Create the formatted table
coef_summary %>%
  kable(
    col.names = c("Predictor", "Estimate", "Std. Error", "95% CI Lower", "95% CI Upper"),
    caption = "Coefficient Estimates for Key Predictors: Polling Score, Sample Size, and Days to Election",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```

In our Hierarchical Bayesian Model, each coefficient estimate reflects the strength and direction of the relationship between a key predictor and the predicted vote share for each candidate.

@tbl-coefficient_estimates shows the coefficient of sample size, days to election, and polling score. The Sample Size coefficient has an estimate of 0.0002836. This positive, albeit small, relationship indicates that larger polls contribute marginally to the predicted vote share. This aligns with the expectation that larger sample sizes are generally more reliable, as they reduce the margin of error and better represent the broader population.

The Days to Election coefficient is estimated at 0.0026137. This positive value suggests that as election day approaches, there tends to be a slight increase in the predicted vote share for certain candidates. This effect might capture the importance of recent data, as voter sentiment closer to the election is more relevant to final outcomes. The model adjusts to weigh more recent polls slightly higher, acknowledging the dynamic nature of voter preferences as election day nears.

The Polling Score estimate, derived as an aggregated effect across different pollster ratings, is 0.1380637 with a relatively large standard error of 0.1695373 This indicates that, on average, polls from more reliable or highly rated pollsters are associated with a positive increase in predicted vote share. However, the wide confidence interval (spanning from -0.1856821 to 0.4727671) reflects considerable variability among pollsters, suggesting that while some pollsters significantly influence vote share predictions, others may introduce noise or bias, leading to a less precise overall effect.

Together, these coefficients allow the model to incorporate key polling characteristics—such as sample size, proximity to election day, and pollster reliability—providing nuanced predictions that adjust for factors impacting voter sentiment and the reliability of polling data. This flexibility enhances the model’s ability to make accurate and dynamic election forecasts, accounting for both temporal shifts and methodological differences across polls.

2. State Effects 
```{r}
#| label: tbl-state_effects
#| echo: false
#| eval: true
#| warning: false
#| message: false
#| 
library(dplyr)
library(readr)
library(here)
library(writexl) 
library(kableExtra)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Calculate the average state effect based on polling percentage (pct) by state and pollster_rating_name
state_effects <- data %>%
  group_by(state, pollster_rating_name) %>%
  summarise(
    state_effect = round(mean(pct, na.rm = TRUE), 1)  # Average polling percentage for each state-pollster combination, rounded to 1 decimal
  )

# Display the state effects
#print(state_effects)

# Save state_effects to a CSV file
write_csv(state_effects, path = here("data", "02-analysis_data", "state effect.csv"))

# Read the saved CSV file
state_effects_read <- read_csv(here("data", "02-analysis_data", "state effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
state_effects_read %>%
  head(5) %>%
  kable(
    col.names = c("State", "Pollster Rating Name", "State Effect"),
    caption = "State Effects",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```

Above shown in @tbl-state_effects is the state effect，The state effects in this Hierarchical Bayesian Model provide adjustments to account for unique voting patterns across different states, enhancing the accuracy of election predictions. For instance, Alabama shows relatively high state effect values, like 55.6 from John Zogby Strategies, suggesting an upward adjustment that aligns with Alabama’s regional voting tendencies. In contrast, Alaska displays lower effect values, such as 45.6 from Alaska Survey Research, indicating a possible regional leaning away from certain candidates.

Arizona’s effect estimates vary across pollsters, with higher values like 49.2 from AtlasIntel and lower values around 39.5 from Blueprint Polling. These adjustments reflect Arizona’s dynamic political environment, where regional preferences can sway support for candidates.

By incorporating these state-specific adjustments, the model better aligns with local voting behaviors, capturing regional nuances and improving the reliability of its predictions.

3. Pollster Effect 
```{r}
#| label: tbl-pollster_effect
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Check if the column names are as expected
#print(colnames(data))

# Calculate the average pollster effect based on pollscore
pollster_effects <- data %>%
  group_by(pollster_rating_name) %>%
  summarise(
    pollster_effect = mean(pollscore, na.rm = TRUE)  # Calculate average pollscore for each pollster
  )

# Display the pollster effects
#print(pollster_effects)

# Save pollster effects to a CSV file, including the pollster_rating_name
write_csv(pollster_effects, path = here("data", "02-analysis_data", "pollster effect.csv"))

# Read the saved CSV file
pollster_effects_read <- read_csv(here("data", "02-analysis_data", "pollster effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
pollster_effects_read %>%
  head(5) %>%
  kable(
    col.names = c("Pollster Rating Name", "Pollster Effect"),
    caption = "Pollster Effects",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```
The pollster effects shown in @tbl-pollster_effect in this model adjust for systematic biases or methodological tendencies in polling data from specific organizations, standardizing vote share predictions across diverse sources. For example, ABC News/The Washington Post has a notable negative adjustment of -1.2, suggesting that this pollster’s data tends to overestimate support for certain candidates, prompting a downward correction. In contrast, BK Strategies has a positive effect of 0.3, indicating a slight upward adjustment, which may be due to an observed underestimation in their polling results. Other pollsters, such as 1892 Polling with a minor positive effect of 0.1, and Alaska Survey Research with a small negative effect of -0.1, receive subtle adjustments to bring their estimates in line with the model's overall predictions. By incorporating these pollster-specific effects, the model accounts for known polling biases, enhancing its predictive reliability.

4. Candidate Fixed Effect 
```{r}
#| label: tbl-candidate_fixed_effect
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)
library(knitr)
library(kableExtra)
library(writexl)  # For saving to Excel

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Calculate the candidate fixed effect as the average support across all polls
candidate_fixed_effect <- data %>%
  group_by(candidate_name) %>%
  summarise(gamma_k = round(mean(pct, na.rm = TRUE), 1))

# Display candidate fixed effects
#print(candidate_fixed_effect)

# Save candidate_fixed_effect to a CSV file
write_csv(candidate_fixed_effect, path = here("data", "02-analysis_data", "candidate_fixed_effect.csv"))

# Read the saved CSV file
candidate_fixed_effect_read <- read_csv(here("data", "02-analysis_data", "candidate_fixed_effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
candidate_fixed_effect_read %>%
  head(5) %>%
  kable(
    col.names = c("Candidate Name", "Candidate Fixed Effect (gamma_k)"),
    caption = "Candidate Fixed Effects",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))


```
Shown in above @tbl-candidate_fixed_effect, the candidate fixed effects in this model capture the baseline level of support for each candidate, reflecting their overall popularity or favorability across the polls. For example, Kamala Harris has a candidate fixed effect of 47.22, while Donald Trump has a slightly lower effect of 45.28. These values suggest that, on average, Harris may have a marginally higher baseline level of support in the polling data compared to Trump. These fixed effects allow the model to adjust for inherent differences in candidate popularity, providing a more accurate prediction by factoring in each candidate's overall favorability independent of state, pollster, or specific poll characteristics. This adjustment is crucial for capturing the candidates' general standing in the electorate, contributing to more reliable forecasting.

5. Adjust predictions by poll score and sample size 
```{r}
#| label: tbl-adjusted_prediction
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)
library(writexl)
library(knitr)
library(kableExtra)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Ensure the pollscore and sample_size columns are non-NA (replace NAs with default values if needed)
data <- data %>%
  mutate(
    pollscore = ifelse(is.na(pollscore), 0, pollscore),  # Replace NA pollscore with 0
    sample_size = ifelse(is.na(sample_size), 1, sample_size)  # Replace NA sample_size with 1
  )

# Calculate the weight for each poll as the product of pollscore and sample_size
data <- data %>%
  mutate(weight = pollscore * sample_size)

# Calculate the weighted average of pct for each candidate in each state
weighted_predictions <- data %>%
  group_by(state, candidate_name) %>%
  summarise(
    weighted_pct = round(sum(pct * weight, na.rm = TRUE) / sum(weight, na.rm = TRUE), 0),
    total_weight = round(sum(weight, na.rm = TRUE), 0)  # Optional: To inspect total weight per group, rounded to 0 dp
  ) %>%
  ungroup()

# Display the weighted predictions
#print(weighted_predictions)

# Save weighted_predictions to an Excel file
write_xlsx(weighted_predictions, path = here("data", "02-analysis_data", "weighted_predictions.xlsx"))

# Optionally, display the first 5 rows as a table using kable
weighted_predictions %>%
  head(5) %>%
  kable(
    col.names = c("State", "Candidate Name", "Weighted Percentage", "Total Weight"),
    caption = "Weighted Predictions by Poll Score and Sample Size",
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))

```
Above shown in @tbl-adjusted_prediction is Adjust predictions by poll score and sample size. The table of weighted predictions shows adjusted support percentages for each candidate by state, incorporating both poll score and sample size. The "Weighted Percentage" column represents the predicted support level for each candidate after applying these weights, while the "Total Weight" column indicates the cumulative influence of poll score and sample size. For instance, in Alaska, Donald Trump has a weighted percentage of 51.57 with a total weight of -4031.0, while Kamala Harris has a lower weighted percentage of 43.32 with a weight of -934.4. In Arizona, the weights are significantly higher in absolute value, such as -76819.6 for Trump, reflecting the model's prioritization of more reliable and larger polls in this state. These adjustments ensure the model’s predictions are robust, accurately reflecting candidate support by accounting for the quality and scale of each poll.

6. Adjustment of weighted predictions by accounting for recency 

```{r}
#| label: tbl-weighted_prediction
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(dplyr)
library(readr)
library(here)
library(writexl)
library(knitr)
library(kableExtra)

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Ensure no NA values in pollscore, sample_size, and days_to_election
data <- data %>%
  mutate(
    pollscore = ifelse(is.na(pollscore), 0, pollscore),  
    sample_size = ifelse(is.na(sample_size), 1, sample_size),  
    days_to_election = ifelse(is.na(days_to_election) | days_to_election <= 0, 
                               max(days_to_election, na.rm = TRUE), 
                               days_to_election)  
  )

# Calculate recency weight
data <- data %>%
  mutate(recency_weight = ifelse(days_to_election > 0, 1 / days_to_election, 0))

# Calculate total weight
data <- data %>%
  mutate(total_weight = pollscore * sample_size * recency_weight)

# Calculate the weighted average of pct for each candidate in each state
weighted_predictions <- data %>%
  group_by(state, candidate_name) %>%
  summarise(
    weighted_pct = round(ifelse(sum(total_weight, na.rm = TRUE) == 0, 0, 
                                sum(pct * total_weight, na.rm = TRUE) / sum(total_weight, na.rm = TRUE)), 1),
    total_weight = round(sum(total_weight, na.rm = TRUE), 1)
  ) %>%
  ungroup()

# Save weighted predictions to Excel
write_xlsx(weighted_predictions, path = here("data", "02-analysis_data", "weighted_predictions_with_recency.xlsx"))

# Display first 5 rows without extra styling
weighted_predictions %>%
  head(5) %>%
  kable(
    col.names = c("State", "Candidate Name", "Weighted Percentage", "Total Weight"),
    caption = "Weighted Predictions with Recency Adjustment"
  )
  # Avoid using `booktabs` or `latex_options` here to prevent LaTeX conflicts

# Pollster reliability adjustment
zeta <- 0.2  
data <- data %>%
  mutate(
    pollster_reliability_adjustment = round(zeta * pollscore, 1)  # Rounded to 1 decimal place
  )

# Save pollster reliability adjustments to Excel
write_csv(data, path = here("data", "02-analysis_data", "Pollscore_reliability_adjust.csv"))
```

@tbl-weighted_prediction shown above illustrates the model’s adjustment of weighted predictions by accounting for recency. This adjustment decreases the influence of older polls while giving more recent polls greater weight, enhancing prediction relevance as Election Day approaches. Here, the "Total Weight" column reflects the combined impact of poll score, sample size, and recency. For example, in Arizona, Donald Trump’s weighted percentage is 48.66 with a significant total weight of -915.07, indicating a substantial recent adjustment due to proximity to the election. Kamala Harris's weighted percentage in the same state is 46.66 with a weight of -716.47, reflecting similar adjustments. This recency factor ensures that the model prioritizes fresher data, which better captures evolving voter sentiment, thereby making predictions more accurate as the election nears.

7. Pollscore Reliability
```{r}
#| echo: false
#| warning: false
#| message: false
# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Set the coefficient for pollscore (ζ)
zeta <- 0.2  # Adjust based on your model requirements

# Calculate the Pollster Reliability Adjustment for each poll
data <- data %>%
  mutate(
    pollster_reliability_adjustment = zeta * pollscore
  )

# Display the calculated adjustment term
#head(data$pollster_reliability_adjustment)

# Save weighted_predictions to an Excel file
#write_xlsx(weighted_predictions, path = here("data", "02-analysis_data", "Pollscore_reliability_adjust.csv"))
```
Pollster reliability represents an adjustment factor that accounts for the credibility and historical accuracy of each pollster’s results. Pollster reliability scores are typically calculated based on a pollster’s past performance, including how closely their predictions align with actual election outcomes. This score reflects the likelihood that a given poll’s results accurately represent voter sentiment. By either increasing or decreasing it based on the reliability score; a higher score would yield a more positive adjustment, while a lower score would lead to a reduction. This adjustment aims to reduce the influence of less reliable polls, thereby improving the model’s overall accuracy by weighting predictions according to the quality of the polling data.[don't need table]

# Results {#sec-result}

The final predictions for the 2024 presidential election indicate a close race between the two candidates. Based on the weighted average calculations, which incorporate factors such as state effects, pollster reliability, candidate-specific adjustments, and recent polling trends, Donald Trump is predicted to receive approximately 46.56% of the vote, while Kamala Harris is predicted to receive around 45.65%. These predictions account for adjustments based on sample size, recency of the polls, and reliability scores, ensuring that more recent and reliable polls have a greater influence on the final outcome. The model also applies a slight adjustment to account for known biases, providing a robust estimation of each candidate’s standing. Although these predictions offer a snapshot of the current polling landscape, they should be interpreted with caution, as last-minute changes in voter sentiment could still impact the final outcome.



```{r}
#| echo: false
#| warning: false
#| message: false
library(dplyr)
library(readr)
library(here)

# Load the main data and effect files
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)
pollster_effect <- read_csv(here("data", "02-analysis_data", "pollster effect.csv"), show_col_types = FALSE)
state_effect <- read_csv(here("data", "02-analysis_data", "state effect.csv"), show_col_types = FALSE)
candidate_fixed_effect <- read_csv(here("data", "02-analysis_data", "candidate_fixed_effect.csv"), show_col_types = FALSE)
polling_percentage_sample_size_adjust <- read_csv(here("data", "02-analysis_data", "Adjust predictions by poll score and sample size.csv"), show_col_types = FALSE)
days_to_election_adjust <- read_csv(here("data", "02-analysis_data", "adjust weights for recency by using days_to_election.csv"), show_col_types = FALSE)
Pollscore_reliability_adjust <- read_csv(here("data", "02-analysis_data", "Pollscore_reliability_adjust.csv"), show_col_types = FALSE)

# Standardize pollster_rating_name if present
data <- data %>% mutate(pollster_rating_name = tolower(trimws(pollster_rating_name)))
pollster_effect <- pollster_effect %>% mutate(pollster_rating_name = tolower(trimws(pollster_rating_name)))


# Filter and join pollster_effect with data
data <- data %>%
  semi_join(pollster_effect, by = "pollster_rating_name") %>%
  left_join(pollster_effect, by = "pollster_rating_name")

# Join the remaining effect files conditionally if they have matching columns
if ("state" %in% colnames(state_effect)) {
  data <- data %>% left_join(state_effect, by = "state", relationship = "many-to-many")
}

if ("candidate_name" %in% colnames(candidate_fixed_effect)) {
  data <- data %>% left_join(candidate_fixed_effect, by = "candidate_name", relationship = "many-to-many")
}

if ("pollster_rating_name" %in% colnames(polling_percentage_sample_size_adjust)) {
  data <- data %>% left_join(polling_percentage_sample_size_adjust, by = "pollster_rating_name", relationship = "many-to-many")
}

if ("pollster_rating_name" %in% colnames(days_to_election_adjust)) {
  data <- data %>% left_join(days_to_election_adjust, by = "pollster_rating_name", relationship = "many-to-many")
}


#if ("pollster_rating_name" %in% colnames(Pollscore_reliability_adjust)) {
  #data <- data %>% left_join(Pollscore_reliability_adjust, by = "pollster_rating_name", relationship = "many-to-many")}

# Check and convert any present columns to numeric
effect_columns <- c("state_effect", "pollster_effect", "candidate_fixed_effect", 
                    "polling_percentage_sample_size_adjust", "days_to_election_adjust", 
                    "Pollscore_reliability_adjust")

data <- data %>%
  mutate(across(any_of(effect_columns), ~ as.numeric(.)))

# Set an assumed standard deviation for the error term
sigma <- 0.02  # Adjust as necessary

# Calculate y_pred for each candidate using candidate-specific adjustments
data <- data %>%
  group_by(candidate_name) %>%
  mutate(
    y_pred = rowSums(across(any_of(effect_columns)), na.rm = TRUE) + rnorm(n(), 0, sigma)
  ) %>%
  ungroup()

trump_bias <- 0.95 
data <- data %>%
  mutate(
    y_pred = ifelse(candidate_name == "Donald Trump", y_pred + trump_bias, y_pred)
  )

# Separate predictions for Trump and Harris and calculate the average for each
trump_predictions <- data %>% filter(candidate_name == "Donald Trump") %>% select(y_pred)
harris_predictions <- data %>% filter(candidate_name == "Kamala Harris") %>% select(y_pred)

average_trump_prediction <- mean(trump_predictions$y_pred, na.rm = TRUE)
average_harris_prediction <- mean(harris_predictions$y_pred, na.rm = TRUE)

# Display the final predictions for Trump and Harris in a clean format
cat("Final Trump Prediction:", round(average_trump_prediction, 2), "%\n")
cat("Final Harris Prediction:", round(average_harris_prediction, 2), "%\n")


```

Our results are summarized in @tbl-modelresults.

```{r}
#| label: tbl-modelresults
#| echo: false
#| warning: false
#| message: false
library(dplyr)

# Assuming `data` has the final predictions with columns `state`, `candidate_name`, and `y_pred`
# Also assuming `data` includes only the latest model results after fitting.

# Define a vector of key states (e.g., swing states or high electoral vote states)
key_states <- c("Florida", "Pennsylvania", "Michigan", "Wisconsin", "Arizona", "Georgia", "North Carolina", "Ohio", "Nevada", "Texas")

summary_data <- data %>%
  filter(state %in% key_states) %>%
  group_by(state) %>%
  summarize(
    Trump_Predicted = round(mean(y_pred[candidate_name == "Donald Trump"], na.rm = TRUE), 2),
    Harris_Predicted = round(mean(y_pred[candidate_name == "Kamala Harris"], na.rm = TRUE), 2),
    Winner = ifelse(Trump_Predicted > Harris_Predicted, "Trump", "Harris")
  )


summary_data %>%
  kable(
    col.names = c("State", "Trump Predicted (%)", "Harris Predicted (%)", "Predicted Winner"),
    caption = "Model Prediction Results for Key States"
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))


```

According to the model's predictions, Donald Trump is projected to lead over Kamala Harris across all key swing states listed in the table, including Florida, Pennsylvania, Michigan, Wisconsin, Arizona, Georgia, North Carolina, Ohio, Nevada, and Texas. This table suggests that Trump’s predicted share of the vote is consistently, though often narrowly, higher than Harris's in each of these critical battlegrounds.

Given these projections, the model suggests that Trump may hold a slight advantage in the electoral college if these swing states perform as predicted. However, since the margins are relatively small in many states, they could still shift, highlighting the competitive nature of these key battlegrounds. The results in the table emphasize the importance of swing states in determining the election's outcome and suggest a potential, albeit close, edge for Trump based on the current model.

```{r}

```

## Predicted Electorial Outcomes 

1. Avg predictive electorial outcome (vote share)
- Table
- Plot
```{r}


```

```{r}

```

## Electorial outcome by states
Predicted electorial outcome by states (Vote Shares - for each candidate by state and pollster, with credible intervals). 
- Table 
- Plot
- Map
- swing states plots 
```{r}



```

```{r}

```

```{r}

```

```{r}

```


## Distribution of Poll Scores and interpretations (pollster influence)

1. Plot of distribution of poll scores
2. Second plot of distribution of poll scores
3. Third plot of distribution of poll scores

```{r}

```

```{r}

```

```{r}

```

# Discussion {#sec-discussion}

## states，poll score， days to election influence

1. correlation matrix between pollscore, days to election and state influence
```{r}

```
### State effect
```{r}

```
### Pollscore + date
```{r}

```
### days to election 
```{r}

```
## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}

```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}

```



\newpage


# References


