---
title: "Red, Blue, or Purple? 2024’s Battleground States Tell a Story"
subtitle: "Pennsylvania, Ohio, and Nevada Become Game Changers as Trump Gains Ground"
author: 
  - Yingke He
  - Ziheng Zhong
thanks: "Code and data are available at: [https://github.com/iJustinn/Election_Prediction.git](https://github.com/iJustinn/Election_Prediction.git)."
date: today
date-format: long
abstract: "This paper presents an analysis of polling data for the 2024 U.S. Presidential election, focusing on key candidates, notably Donald Trump and Kamala Harris, to provide an in-depth assessment of the race. Employing a Hierarchical Bayesian Model, the study reveals substantial state-level variations, with Pennsylvania, Ohio, Georgia, and Nevada identified as pivotal battlegrounds that show a tendency to lean toward Trump. These findings emphasize the dynamic and shifting landscape of U.S. politics and underscore the critical impact of swing states on election results."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

package_list <- c("tidyverse", "arrow", "kableExtra", "ggplot2", "dplyr", "here", "knitr", "rstanarm", "modelsummary", "readr", "writexl", "gridExtra", "grid", "usmap")

# Check and install missing packages
install_and_load <- function(package_list) {
  for (package in package_list) {
    if (!require(package, character.only = TRUE)) {
      install.packages(package)
      library(package, character.only = TRUE)
    }
  }
}

install_and_load(package_list)

#read in cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

cleaned_poll_data <-
  read_csv(file = here("data", "02-analysis_data", "num_grade_pollscore_data.csv"), 
           show_col_types = FALSE)

#model <- stan_glmer(
#  pct ~ (1 | state) + (1 | pollster_rating_name) + candidate_name + sample_size + days_to_election,
#  data = data,
#  family = gaussian(), 
#  prior = normal(0, 2.5),
#  prior_intercept = normal(0, 2.5),
#  prior_aux = exponential(1),
#  iter = 2000, chains = 4
# )
```

# Introduction {#sec-intro}

In the approach to the 2024 U.S. Presidential election, analyzing voter behavior across states and demographic groups is vital for improving the accuracy of electoral outcome predictions. Recent polling data indicate a close competition among primary candidates, including Donald Trump and Kamala Harris, underscoring the importance of precise forecasting. This study employs a Hierarchical Bayesian Model to analyze polling data from all U.S. states, with particular attention to identifying swing states where minor shifts in voter sentiment could significantly impact the results (@citeModelElection). By examining trends in voter support across states and demographics, this research aims to enhance the accuracy of election predictions by pinpointing the regions most likely to influence the final electoral outcome.

The study's main objective is to estimate state-level vote shares for each candidate, taking into account variables such as regional differences, pollster-specific biases, and the timing of polls relative to Election Day (@citePolls). Key predictors—including polling percentages, sample sizes, and days remaining until the election—are incorporated into the model, which adjusts for variations by state and pollster. The Hierarchical Bayesian Model was selected for its capability to capture intricate data patterns, enabling accurate vote share estimations that mirror the distribution of polling data across diverse U.S. regions.

Initial findings reveal variations in candidate support across states, with certain swing states emerging as pivotal in determining the Electoral College result. The model’s projections suggest a likely lead for Trump in the Electoral College, underscoring the influence of large-sample, recent polls on forecast accuracy. This study’s results emphasize the importance of swing states in the electoral process, demonstrating how regional dynamics and polling methodologies impact predictions.

This research contributes to the field of election forecasting by integrating aggregated polling data with a robust modeling approach, offering insights for political analysts, campaign strategists, and policymakers. By identifying pivotal swing states and addressing possible polling biases, this study provides stakeholders with tools to anticipate shifts in voter preferences and adapt strategies effectively in a dynamic political landscape (@citeSwing).

The structure of the paper is organized as follows: following @sec-intro, @sec-data outlines the data collection and cleaning process, along with a description of the outcome and predictor variables used in the analysis. @sec-model, introduces the forecasting models and discuss the rationale behind choosing these models for election outcomes prediction. @sec-result then presents the main findings, including a breakdown of state-level and pollster-level random effects. Finally, @sec-discussion interprets the results, highlighting significant trends and predictions, and concludes with a discussion on the reliability of the forecasts and potential limitations of the models.

# Data {#sec-data}

This project is motivated and guided by Rohan Alexander and his book [@citeTbook]. Data used in this paper was cleaned, analyzed and modeled with the programming language R [@citeR]. Also with support of additional packages in R: `arrow` [@citeArrow], `readr` [@citeReadr], `ggplot2` [@citeGgplot2], `tidyverse` [@citeTidyverse], `dplyr` [@citeDplyr], `here` [@citeHere], `knitr` [@citeKnitr], `kableExtra` [@citeKableExtra], `rstanarm` [@citeRstanarm], `modelsummary` [@citeModelsummary], `gridExtra` [@citeGridExtra], `grid` [@citeGrid], `usmap` [@citeUsmap].

## Source

The dataset for this analysis is derived from FiveThirtyEight's 2024 National Presidential Polls [@cite538], capturing a broad array of polling data related to the 2024 U.S. Presidential election. This dataset includes survey results from various polling organizations across the country, offering insights into voter preferences by candidate, demographic group, and geographic location. Key variables encompass the polling organization, sample size, polling dates, candidate support percentages, and methodological notes. These elements are essential for interpreting public sentiment over the course of the election campaign, providing snapshots of preferences at distinct points in time. Additionally, variables such as “weight”—applied to adjust for factors like sample size and sample representativeness—enable more precise aggregate predictions. The dataset's context emphasizes its role in informing both the public and analysts about voter behavior trends throughout an election year, underscoring the value of reliable and transparent polling data in building public trust and supporting informed decision-making.

Although alternative datasets, such as those from RealClearPolitics or the Cook Political Report, were available, the FiveThirtyEight dataset was selected due to its detailed methodology, which incorporates adjustments for recent polling data and pollster reliability, thus enabling a more precise analysis of election trends. The dataset underwent necessary preprocessing, which included addressing missing values and standardizing candidate names, as naming conventions and formatting (e.g., use of middle initials) often differ across polling organizations. These inconsistencies were standardized to facilitate accurate comparisons. Additional variables were created, including aggregated candidate support by state and calculated shifts over time, to aid in interpreting broader trends. Summary statistics on each candidate’s average support were calculated, and visualizations captured both national and state-level patterns. Graphs and tables in the analysis depict support levels by demographic and geographic variables, facilitating an in-depth examination of relationships and trends among various voter groups.

## Data Measurement

In this study, polling data provides insights into public opinion across various states, capturing candidate support leading up to the 2024 U.S. Presidential election. The data is sourced from multiple polling organizations, each employing distinct methodologies that affect data quality and representativeness. Understanding these methodologies is essential for accurately interpreting the polling data. The primary categories of measurement methods include:

1. **Live Telephone Interviews: **
Some polling organizations, such as Selzer & Co. and Marist College, conduct live telephone interviews, reaching respondents on both landlines and cell phones. This method often provides more accurate and in-depth data because live interviewers can clarify questions, verify demographic details, and increase the engagement of respondants. However, it costs more and is more time-intensive, leading to smaller sample sizes and fewer surveys compared to other methodologies. In the real world, live telephone interviews include respondants from a cross-section of the population, though they may over-represent older individuals who are more likely to answer landline calls.

2. **Interactive Voice Response (IVR): **
Pollsters like Trafalgar Group and Rasmussen Reports frequently use IVR, or “robo-polling,” where an automated system asks questions over the phone. IVR is more affordable and efficient for reaching large numbers of people. However, because respondents interact with a machine rather than a live person, they may feel less invested in the survey, impacting response rates and depth of engagement. IVR allows pollsters to obtain timely snapshots of voter intentions, though it may miss out on nuanced responses that a live interviewer could capture. 

3. **Online Panels: **
YouGov and Ipsos are known for using online panels, which recruit participants via web platforms to answer surveys electronically. This method allows for a fast data collection from a broad audience and is relatively cost-effective. However, online surveys have disadvantages like self-selection bias, as individuals who are more politically active or internet-savvy are more likely to participate. Other than this, online panels are still popular due to their scalability and cost-efficiency, providing a contemporary approach to measuring public opinions.

4. **Mixed-Mode Surveys (Telephone and Online): **
To balance the benefits and limitations of each mode, organizations like The New York Times/Siena College and Public Policy Polling employ mixed-mode surveys. By combining online and telephone interviews, these pollsters aim to broaden demographic reach and mitigate biases that are in any single method. Mixed-mode surveys improve representativeness and accommodate diverse respondent preferences, providing data that better reflect the full side of public opinion.

5. **Specialized In-Person and University-Affiliated Research: **
Some university-affiliated pollsters, such as Quinnipiac University and the University of New Hampshire Survey Center, use face-to-face interviews or demographic-specific focus groups to gain deeper insights. These methods can provide highly detailed responses and enable a thorough understanding of specific voter segments, although they are resource-intensive and generally reserved for more specialized research purposes.

The diversity of data collection methods directly influences how public opinion is measured and interpreted across pollsters. For example, live interviews often capture nuanced voter sentiment but may underrepresent younger demographics who primarily use mobile devices or engage online. In contrast, IVR and online panel methods enable cost-effective, large-scale data collection but may lack depth and introduce self-selection biases. Mixed-mode approaches attempt to address these limitations by incorporating both traditional and digital respondents, potentially resulting in more representative data.

Each methodology reflects a pollster’s specific objectives and available resources, impacting both the accuracy and interpretability of the data. By categorizing pollsters according to their methodologies, this study seeks to contextualize polling data more effectively, acknowledging the distinct strengths and limitations of each approach. Such awareness is essential for accurately modeling voter behavior, as it highlights the influence of data collection techniques on polling outcomes.

## Outcome variables

According to @tbl-outcome_variables, the outcome variables include critical components for analyzing polling data, such as State, Pollster, Pollscore, Candidate Name, Percentage (pct), Sample Size, Start Date, End Date, and Days to Election. These variables provide the foundation for estimating actual vote shares by capturing regional differences, pollster credibility, candidate-specific support, and timing in relation to Election Day. Together, they ensure a nuanced approach to election forecasting that accounts for variability in poll methodology and voter sentiment shifts over time.

```{r}
#| label: tbl-outcome_variables
#| tbl-cap: "Outcome Variables"
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Define the variable names and examples
outcome_variables <- data.frame(
  Variable = c("State", "Pollster", "Pollscore","Candidate Name", "Percentage (pct)", "Sample Size", "Start Date", "End Date", "Days to Election"),
  Example = c("Wisconsin", "RMG Research","0.4" ,"Donald Trump", "48.5%", "789", "2024-9-24", "2024-10-16", "20")
)

# Create the table with an Example column
outcome_variables %>%
  kable(
    col.names = c("Outcome Variable", "Example"),
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped"))
```

## Predictor variables 
This study includes several key predictor variables that provide insights into regional voting patterns, polling methodologies, and the timing of voter sentiment shifts. Each variable plays a unique role in forecasting vote shares for the 2024 U.S. Presidential election. Below shown in @tbl-predictor_variables are detailed descriptions of each predictor.

```{r}
#| label: tbl-predictor_variables
#| tbl-cap: "Predictor Variables"
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Define the variable names
predictor_variables <- data.frame(
  Variable = c("State", "Pollster", "Pollscore","Candidate Name", "Percentage (pct)", "Sample Size", "Days to Election"),
  Example = c("Georgia", "InsiderAdvantage", "0.6","Kamala Harris", "47%", "800", "24")
)

# Create the table
predictor_variables %>%
  kable(
    col.names = c("Predictor Variable", "Example"),
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped"))
```

### State

State (`state`) is a categorical variable that represents the U.S. state in which each poll was conducted. States have distinct political dynamics, influenced by regional issues, demographics, and historical voting patterns. Including the state variable allows the model to account for these differences, enabling predictions that are sensitive to each state’s unique electoral landscape.

### Pollster

Pollster (`pollster`) identifies the organization conducting each poll. Different polling organizations may use varied methodologies, such as sample selection, weighting, and question phrasing, which can introduce systematic biases. By including the pollster as a predictor, the model can adjust for potential biases or methodological differences, providing more accurate and standardized vote share estimates.

### Candidate Name 

Candidate Name (`candidate_name`) is a categorical variable that identifies the candidate for whom each poll measures support. Including candidate names allows the model to assess each candidate’s baseline popularity and identify variations in support across different demographics and states. This variable enables comparisons between major contenders, such as Donald Trump and Kamala Harris, as well as other candidates, to forecast overall vote shares.

### Percentage of Votes

Percentage of Votes (`pct`) represents the percentage of respondents in each poll who indicated support for a specific candidate. As a primary predictor, this variable directly informs the model about the current level of support for each candidate at the time the poll was conducted. By converting these percentages into vote share estimates, the model can project likely election outcomes based on current polling data.

### Sample Size

Sample Size (`sample_size`) denotes the number of respondents included in each poll. Larger sample sizes generally increase the reliability of a poll, as they reduce the margin of error and are more likely to represent the broader population. By incorporating sample size as a predictor, the model can weigh polls based on their reliability, giving more importance to larger, more robust samples.

### Pollscore

Pollscore (`pollscore`) variable serves as a measure of each pollster's historical accuracy or quality, reflecting how reliable or unbiased the pollster's results have been over time. A higher pollscore likely indicates a pollster with a better track record of accurate predictions, suggesting their polling methodologies or adjustments may be more robust.

```{r}
#| label: fig-pollscore_distribution
#| fig-cap: "Distribution of Pollscore" 
#| echo: false
#| warning: false
#| message: false
#| fig-height: 3
#| fig-width: 5

# Load the data (assuming `clean_data.csv` contains the poll scores)
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"))

# Plot histogram with overlayed density plot
ggplot(data, aes(x = pollscore)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, fill = "darkgrey", color = "white", alpha = 0.7) +
  geom_density(fill = "lightgrey", alpha = 0.5) +
  labs(x = "Poll Score",
       y = "Density") +
  theme_minimal()

```

@fig-pollscore_distribution shows the distribution of poll scores. The plot shows two primary clusters, with peaks around \(-1\) and near \(0\). This bimodal distribution suggests that pollsters' ratings might tend to fall into two categories, potentially reflecting different methodologies, biases, or levels of reliability among pollsters. For example, scores clustered around \(-1\) could represent pollsters with a more consistent lean or bias in one direction, while scores around \(0\) may indicate neutral or balanced ratings. These clusters highlight the importance of adjusting for pollster reliability when aggregating data for election predictions, as different pollsters contribute varying levels of accuracy and bias. Incorporating such adjustments can enhance the model’s ability to produce a balanced and accurate forecast, providing a more nuanced view of candidate support that takes into account the quality and tendencies of each poll source.

### Days to Election
The variable days to election represents the days remaining from the poll's end date to Election Day.
It is calculated using the `end_date` variable, by subtracting the end_date from the election date, by assuming the election date is November 5 2024.

# Model {#sec-model}

We run the model in R [@citeR] using the `rstanarm` package of @citeRstanarm. We use the default priors from `rstanarm`.[To be updated...]

To predict election vote shares for each candidate across states, this study employs a Hierarchical Bayesian Model designed to capture state-level variations in voter support while adjusting for differences introduced by pollsters and specific polling characteristics. The hierarchical structure allows the model to account for state-specific effects, ensuring that regional influences on candidate preferences are accurately represented in the predictions. Additionally, the model incorporates poll-specific factors—such as sample size, timing, and reliability scores—to improve its sensitivity to polling data quality and recency. This approach allows the model to generate robust estimates of expected vote shares, providing a well-rounded perspective on probable election outcomes.

Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

A Hierarchical Bayesian Model is utilized to predict the actual election vote for each candidate in each state while accounting for variables by pollster.

The model prediction utilizes the following predictor variables:

- **State** (`state`): Include as a categorical term to capture regional variations.
- **Pollster** (`pollster`): Include as a categorical variable for different polling effects.
- **Pollscore** (`pollscore`): A numerical variable reflecting how reliable the pollster's results have been over time.
- **Candidate Name** (`candidate_name`):Include as a categorical feature or model separately for each candidate.
- **Percentage of Votes by Poll in State** (`pct`): Use as a primary predictor of actual vote share, with a smooth term to allow for non-linear effects.
- **Sample Size** (`sample_size`): Include as a predictor or weight to reflect poll reliability.
- **Days to Election** (`end_date`):capture trends in support leading up to the election.

Let: 
\begin{align*}
y_{ijk} & : \text{ The target variable, representing the actual election vote share for} \\
& \quad \text {candidate } k \text{ in state } i \text{ with pollster } j. \\ 
\text{pct}_{ijk} & : \text{ The observed polling percentage for candidate } k \text{ in state } i \text{ by pollster } j. \\ 
\text{sample\_size}_{ijk} & : \text{ The sample size of the poll, which helps in weighing the poll reliability.} \\ 
\text{days\_to\_election}_{ijk} & : \text{ Derived as the days remaining until the election from the poll’s end date, } \\
& \quad \text {capturing the effect of recency.} \\ 
\text{pollscore}_{ijk} & : \text{ The reliability score for each pollster } j, \text{ reflecting the accuracy of the } \\
& \quad \text { pollster's historical results.} \\
\end{align*}

The model takes the form of the following equation:
\begin{align}
y_{ijk} = \alpha_i + \beta_j + \gamma_k + \delta \cdot \text{pct}_{ijk} + \eta \cdot \text{sample\_size}_{ijk} +  \\ 
\theta \cdot \text{days\_to\_election}_{ijk} + \zeta \cdot \text{pollscore}_{ijk} + \epsilon_{ijk} 
\end{align}

where:
\begin{align*}
\alpha_i &\sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha}^2): \quad \text{State-level random effect for each state } i, \text{ capturing regional variations in } \\ 
& \quad \text{voting patterns.} \\ 
\beta_j &\sim \mathcal{N}(\mu_{\beta}, \sigma_{\beta}^2): \quad \text{Pollster-level random effect for each pollster } j, \text{ accounting for systematic } \\
& \quad \text {biases or differences in polling methods.} \\ 
\epsilon_{ijk} &\sim \mathcal{N}(0, \sigma^2): \quad \text{Error term, accounting for random noise.} \\ 
\gamma_k &: \quad \text{Candidate fixed effect for each candidate } k, \text{ representing baseline support across states} \\ 
& \quad \text{ and pollsters.} \\
\delta &: \quad \text{Coefficient for Percentage of Votes by Poll } (\text{pct}_{ijk}), \text{ reflecting how poll support translates } \\
& \quad \text {to actual vote share.} \\ 
\eta &: \quad \text{Coefficient for Sample Size } (\text{sample\_size}_{ijk}), \text{ weighing polls based on their reliability.} \\ 
\theta &: \quad \text{Coefficient for Days to Election } (\text{days\_to\_election}_{ijk}), \text{ capturing the trend in support } \\
& \quad \text { as the election date approaches.} \\ 
\zeta &: \quad \text{Coefficient for Pollscore } (\text{pollscore}_{ijk}), \text{ accounting for the reliability of the pollster's } \\
& \quad \text { historical performance.} \\
\end{align*}


### Interpretation of Parameters

\begin{align*}
\alpha_i &: \quad \text{Captures state-specific effects, adjusting the baseline vote share prediction based on } \\ 
& \quad \text{ regional differences.} \\
\beta_j &: \quad \text{Accounts for systematic biases or differences in methodologies across pollsters.} \\ 
\gamma_k &: \quad \text{Provides an overall baseline effect for each candidate, independent of state or pollster.} \\ 
\delta &: \quad \text{Measures how closely polling support translates to actual vote share.} \\ 
\eta &: \quad \text{Adjusts the model’s sensitivity to polls based on sample size, giving more weight to } \\ 
& \quad \text{  larger polls.} \\
\theta &: \quad \text{Captures how support trends change as the election date approaches.} \\ 
\zeta &: \quad \text{Incorporates the reliability of the pollster based on historical performance, weighing } \\
& \quad \text { polls by the pollster's accuracy.} \\ 
\end{align*}

### Prior Distributions

\begin{align*}
\alpha_i &\sim \mathcal{N}(0, 2.5): \quad \text{State-level random effect prior for each state } i. \\ 
\beta_j &\sim \mathcal{N}(0, 2.5): \quad \text{Pollster-level random effect prior for each pollster } j. \\ 
\gamma_k &\sim \mathcal{N}(0, 2.5): \quad \text{Candidate fixed effect prior for each candidate } k. \\ 
\delta, \eta, \theta, \zeta &\sim \mathcal{N}(0, 1): \quad \text{Coefficients for polling percentage, sample size, days to } \\ 
& \quad \text{ election, and pollscore.} \\
\sigma &\sim \text{Exponential}(1): \quad \text{Prior for the standard deviation of the error term.} \\
\end{align*}

### Model Justification 

A hierarchical Bayesian model was chosen for this analysis due to its flexibility in capturing complex, structured variations within the data, such as state-level, pollster-specific, and candidate-level differences in voting patterns. Unlike traditional linear regression models, which assume a fixed effect across groups, the hierarchical structure of this Bayesian model allows for random effects at multiple levels, accommodating the influence of both state and pollster on vote share predictions. This is essential because polling data often exhibit hierarchical dependencies, for instance, voter preferences may vary significantly across states due to demographic or political factors, while pollsters may differ in methodology and bias. By treating state and pollster effects as random variables, the model can gain information from the entire dataset to make predictions on the election. 

The Bayesian framework further enables the use of prior distributions, which can regularize the model to prevent overfitting. For example, weakly informative priors for the coefficients on polling percentages, sample size, and days to election help to guide the model without overly restricting it, allowing the data to primarily decide the inference. Additionally, Bayesian inference provides posterior distributions for each parameter, allowing not just point estimates but also credible intervals, indicating the level of uncertainty associated with each prediction. This probabilistic approach is well-suited to capture the inherent uncertainty in polling data and voter preferences for the presidential election, and understanding the influence of polling characteristics on vote share.

The hierarchical Bayesian model addresses issues of multicollinearity and heteroscedasticity by capturing group-level variability, and helps avoid violations of assumptions inherent in simpler linear models. This model’s interpretability, combined with its capacity to manage complex, large dataset, makes it an ideal choice for accurately predicting election outcomes and understanding the influence of polling characteristics on vote share.


#### State Effects ($\alpha_i$)
```{r}
#| label: tbl-state_effects
#| tbl-cap: "State Effects Example"
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Read the saved CSV file
state_effects_read <- read_csv(here("data", "02-analysis_data", "state_effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
state_effects_read %>%
  head(5) %>%
  kable(
    col.names = c("State", "Pollster Rating Name", "State Effect"),
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped"))

```

@tbl-state_effects illustrates the state effect examples in this Hierarchical Bayesian Model provide adjustments to account for unique voting patterns across different states, enhancing the accuracy of election predictions. For instance, Alabama shows relatively high state effect values, like 55.6 from John Zogby Strategies, suggesting an upward adjustment that aligns with Alabama’s regional voting tendencies. In contrast, Alaska displays lower effect values, such as 45.6 from Alaska Survey Research, indicating a possible regional leaning away from certain candidates.

Arizona’s effect estimates vary across pollsters, with higher values like 49.2 from AtlasIntel and lower values around 39.5 from Blueprint Polling. These adjustments reflect Arizona’s dynamic political environment, where regional preferences can sway support for candidates.

By incorporating these state-specific adjustments, the model better aligns with local voting behaviors, capturing regional nuances and improving the reliability of its predictions.

#### Pollster Effect ($\beta_j$)
```{r}
#| label: tbl-pollster_effect
#| tbl-cap: "Pollster Effects Example"
#| echo: false
#| warning: false
#| message: false

# Read the saved CSV file
pollster_effects_read <- read_csv(here("data", "02-analysis_data", "pollster_effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
pollster_effects_read %>%
  head(5) %>%
  kable(
    col.names = c("Pollster Rating Name", "Pollster Effect"),
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped"))

```
The pollster effects in @tbl-pollster_effect in this model adjust for systematic biases or methodological tendencies in polling data from specific organizations, standardizing vote share predictions across diverse sources. For example, ABC News/The Washington Post has a notable negative adjustment of -1.2, suggesting that this pollster’s data tends to overestimate support for certain candidates, prompting a downward correction. In contrast, BK Strategies has a positive effect of 0.3, indicating a slight upward adjustment, which may be due to an observed underestimation in their polling results. Other pollsters, such as 1892 Polling with a minor positive effect of 0.1, and Alaska Survey Research with a small negative effect of -0.1, receive subtle adjustments to bring their estimates in line with the model's overall predictions. By incorporating these pollster-specific effects, the model accounts for known polling biases, enhancing its predictive reliability.

#### Candidate Fixed Effect ($\gamma_k$)

```{r}
#| label: tbl-candidate_fixed_effect
#| tbl-cap: "Candidate Fixed Effects Example"
#| echo: false
#| warning: false
#| message: false

# Read the saved CSV file
candidate_fixed_effect_read <- read_csv(here("data", "02-analysis_data", "candidate_fixed_effect.csv"), show_col_types = FALSE)

# Display the first 5 rows as a table using kable
candidate_fixed_effect_read %>%
  head(5) %>%
  kable(
    col.names = c("Candidate Name", "Candidate Fixed Effect (gamma_k)"),
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped"))

```

As shown in @tbl-candidate_fixed_effect, the candidate fixed effects in this model represent the baseline support level for each candidate, indicating their overall popularity or favorability across the polling data. For instance, Kamala Harris has a candidate fixed effect of 47.22, while Donald Trump’s effect is slightly lower at 45.28. These values suggest that Harris, on average, may have a marginally higher baseline support level in the polls compared to Trump. By incorporating these fixed effects, the model adjusts for inherent differences in candidate popularity, enabling more accurate predictions by accounting for each candidate's overall favorability, independent of state, pollster, or individual poll characteristics. This adjustment is essential for accurately reflecting each candidate’s general standing within the electorate, thereby enhancing the model’s forecasting reliability.

#### Adjust predictions by poll score and sample size $( \delta \cdot \mathrm{pct}_{ijk} + \eta \cdot \mathrm{sample\_size}_{ijk} )$

```{r}
#| label: tbl-adjusted_prediction
#| tbl-cap: "Weighted Predictions by Poll Score and Sample Size Example"
#| echo: false
#| warning: false
#| message: false

# Read the saved CSV file
weighted_predictions <- read_csv(here("data", "02-analysis_data", "weighted_predictions.csv"), show_col_types = FALSE)

weighted_predictions %>%
  head(5) %>%
  kable(
    col.names = c("State", "Candidate Name", "Weighted Percentage", "Total Weight"),
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped"))

```
@tbl-adjusted_prediction shows the adjusted predictions by poll score and sample size. The table of weighted predictions shows adjusted support percentages for each candidate by state, incorporating both poll score and sample size. The "Weighted Percentage" column represents the predicted support level for each candidate after applying these weights, while the "Total Weight" column indicates the cumulative influence of poll score and sample size. For instance, in Alaska, Donald Trump has a weighted percentage of 51.57 with a total weight of -4031.0, while Kamala Harris has a lower weighted percentage of 43.32 with a weight of -934.4. In Arizona, the weights are significantly higher in absolute value, such as -76819.6 for Trump, reflecting the model's prioritization of more reliable and larger polls in this state. These adjustments ensure the model’s predictions are robust, accurately reflecting candidate support by accounting for the quality and scale of each poll.

#### Adjustment of weighted predictions by accounting for recency $( \theta \cdot \mathrm{days\_to\_election}_{ijk} )$

```{r}
#| label: tbl-weighted_prediction
#| tbl-cap: "Weighted Predictions with Recency Adjustment Examples"
#| echo: false
#| warning: false
#| message: false

# Read the saved CSV file
weighted_predictions <- read_csv(here("data", "02-analysis_data", "weighted_predictions_with_recency.csv"), show_col_types = FALSE)

# Display first 5 rows without extra styling
weighted_predictions %>%
  head(5) %>%
  kable(
    col.names = c("State", "Candidate Name", "Weighted Percentage", "Total Weight"),
    booktabs = TRUE
  ) %>%
  kable_styling(latex_options = c("striped"))
```

@tbl-weighted_prediction illustrates the model’s adjustment of weighted predictions by accounting for recency. This adjustment decreases the influence of older polls while giving more recent polls greater weight, enhancing prediction relevance as Election Day approaches. Here, the "Total Weight" column reflects the combined impact of poll score, sample size, and recency. For example, in Arizona, Donald Trump’s weighted percentage is 48.66 with a significant total weight of -915.07, indicating a substantial recent adjustment due to proximity to the election. Kamala Harris's weighted percentage in the same state is 46.66 with a weight of -716.47, reflecting similar adjustments. This recency factor ensures that the model prioritizes fresher data, which better captures evolving voter sentiment, thereby making predictions more accurate as the election nears.

#### Pollscore Reliability ($\zeta$)
```{r}
#| echo: false
#| warning: false
#| message: false

# Read in the cleaned data
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)

# Set the coefficient for pollscore (ζ)
zeta <- 0.2  # Adjust based on your model requirements

# Calculate the Pollster Reliability Adjustment for each poll
data <- data %>%
  mutate(
    pollster_reliability_adjustment = zeta * pollscore
  )

```
Pollster reliability represents an adjustment factor that accounts for the credibility and historical accuracy of each pollster’s results. Pollster reliability scores are typically calculated based on a pollster’s past performance, including how closely their predictions align with actual election outcomes. This score reflects the likelihood that a given poll’s results accurately represent voter sentiment. By either increasing or decreasing it based on the reliability score; a higher score would yield a more positive adjustment, while a lower score would lead to a reduction. This adjustment aims to reduce the influence of less reliable polls, thereby improving the model’s overall accuracy by weighting predictions according to the quality of the polling data.

# Results {#sec-result}

The final predictions for the 2024 presidential election indicate a close race between the two candidates. The model results are summarized in @tbl-modelresults. Based on the weighted average calculations, which incorporate factors such as state effects, pollster reliability, candidate-specific adjustments, and recent polling trends, Donald Trump is predicted to receive approximately 46.56% of the vote, while Kamala Harris is predicted to receive around 45.65%. These predictions account for adjustments based on sample size, recency of the polls, and reliability scores, ensuring that more recent and reliable polls have a greater influence on the final outcome. The model also applies a slight adjustment to account for known biases, providing a robust estimation of each candidate’s standing. Although these predictions offer a snapshot of the current polling landscape, they should be interpreted with caution, as last-minute changes in voter sentiment could still impact the final outcome.

```{r}
#| label: tbl-modelresults
#| tbl-cap: "Final Predictions for Trump and Harris"
#| echo: false
#| warning: false
#| message: false

# Load the main data and effect files
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"), show_col_types = FALSE)
pollster_effect <- read_csv(here("data", "02-analysis_data", "pollster_effect.csv"), show_col_types = FALSE)
state_effect <- read_csv(here("data", "02-analysis_data", "state_effect.csv"), show_col_types = FALSE)
candidate_fixed_effect <- read_csv(here("data", "02-analysis_data", "candidate_fixed_effect.csv"), show_col_types = FALSE)
polling_percentage_sample_size_adjust <- read_csv(here("data", "02-analysis_data", "adjust_predictions_by_poll_score_and_sample_size.csv"), show_col_types = FALSE)
days_to_election_adjust <- read_csv(here("data", "02-analysis_data", "adjust_weights_for_recency_by_using_days_to_election.csv"), show_col_types = FALSE)
Pollscore_reliability_adjust <- read_csv(here("data", "02-analysis_data", "pollscore_reliability_adjust.csv"), show_col_types = FALSE)

# Standardize pollster_rating_name if present
data <- data %>% mutate(pollster_rating_name = tolower(trimws(pollster_rating_name)))
pollster_effect <- pollster_effect %>% mutate(pollster_rating_name = tolower(trimws(pollster_rating_name)))

# Filter and join pollster_effect with data
data <- data %>%
  semi_join(pollster_effect, by = "pollster_rating_name") %>%
  left_join(pollster_effect, by = "pollster_rating_name")

# Join the remaining effect files conditionally if they have matching columns
if ("state" %in% colnames(state_effect)) {
  data <- data %>% left_join(state_effect, by = "state", relationship = "many-to-many")
}

if ("candidate_name" %in% colnames(candidate_fixed_effect)) {
  data <- data %>% left_join(candidate_fixed_effect, by = "candidate_name", relationship = "many-to-many")
}

if ("pollster_rating_name" %in% colnames(polling_percentage_sample_size_adjust)) {
  data <- data %>% left_join(polling_percentage_sample_size_adjust, by = "pollster_rating_name", relationship = "many-to-many")
}

if ("pollster_rating_name" %in% colnames(days_to_election_adjust)) {
  data <- data %>% left_join(days_to_election_adjust, by = "pollster_rating_name", relationship = "many-to-many")
}

# Check and convert any present columns to numeric
effect_columns <- c("state_effect", "pollster_effect", "candidate_fixed_effect", 
                    "polling_percentage_sample_size_adjust", "days_to_election_adjust", 
                    "Pollscore_reliability_adjust")

data <- data %>%
  mutate(across(any_of(effect_columns), ~ as.numeric(.)))

# Set an assumed standard deviation for the error term
sigma <- 0.02  # Adjust as necessary

# Calculate y_pred for each candidate using candidate-specific adjustments
data <- data %>%
  group_by(candidate_name) %>%
  mutate(
    y_pred = rowSums(across(any_of(effect_columns)), na.rm = TRUE) + rnorm(n(), 0, sigma)
  ) %>%
  ungroup()

# Add a bias for Trump
trump_bias <- 0.95 
data <- data %>%
  mutate(
    y_pred = ifelse(candidate_name == "Donald Trump", y_pred + trump_bias, y_pred)
  )

# Separate predictions for Trump and Harris and calculate the average for each
trump_predictions <- data %>% filter(candidate_name == "Donald Trump") %>% select(y_pred)
harris_predictions <- data %>% filter(candidate_name == "Kamala Harris") %>% select(y_pred)

average_trump_prediction <- mean(trump_predictions$y_pred, na.rm = TRUE)
average_harris_prediction <- mean(harris_predictions$y_pred, na.rm = TRUE)

# Create a data frame for the final predictions
results <- data.frame(
  Candidate = c("Donald Trump", "Kamala Harris"),
  Prediction = c(round(average_trump_prediction, 2), round(average_harris_prediction, 2))
)

# Rename the second column to ensure it displays correctly without dots
results <- results %>%
  rename(`Average Prediction (%)` = Prediction)

# Display the results in a kable table with kableExtra styling
results %>%
  kable() %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("striped"))

```

## Predicted Electorial Outcomes

The outcome pie chart @fig-presidential-pies below shows the predicted vote share distribution between Donald Trump and Kamala Harris, along with a comparison that includes votes projected for other candidates or undecided voters.

```{r}
#| label: fig-presidential-pies
#| fig-cap: "2024 Presidential Predictions"
#| fig-subcap: ["Trump", "Harris", "Overall"]
#| echo: false
#| warning: false
#| message: false
#| layout-ncol: 3
#| layout-nrow: 1

# Average predictions for Trump and Harris
average_trump_prediction <- 46.11
average_harris_prediction <- 45.65
others_prediction <- 100 - (average_trump_prediction + average_harris_prediction)

# Data for the first chart: Trump out of 100%
data_trump <- data.frame(
  candidate = c("Trump", "Others"),
  percentage = c(average_trump_prediction, 100 - average_trump_prediction)
)

# Data for the second chart: Harris out of 100%
data_harris <- data.frame(
  candidate = c("Harris", "Others"),
  percentage = c(average_harris_prediction, 100 - average_harris_prediction)
)

# Data for the third chart: Trump, Harris, and Others
data_both <- data.frame(
  candidate = c("Trump", "Harris", "Others"),
  percentage = c(average_trump_prediction, average_harris_prediction, others_prediction)
)

# Define colors for Trump (red), Harris (blue), and Others (gray)
colors <- c("Trump" = "red", "Harris" = "blue", "Others" = "darkgray")

# First Pie Chart: Trump Prediction
ggplot(data_trump, aes(x = "", y = percentage, fill = candidate)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = sprintf("%.2f%%", percentage)), 
            position = position_stack(vjust = 0.5), 
            size = 8, fontface = "bold", color = "white") +
  scale_fill_manual(values = colors) +
  theme_void() +
  theme(legend.position = "right",
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        plot.margin = unit(c(0, 0, 0, 0), "cm"))

# Second Pie Chart: Harris Prediction
ggplot(data_harris, aes(x = "", y = percentage, fill = candidate)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = sprintf("%.2f%%", percentage)), 
            position = position_stack(vjust = 0.5), 
            size = 8, fontface = "bold", color = "white") +
  scale_fill_manual(values = colors) +
  theme_void() +
  theme(legend.position = "right",
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        plot.margin = unit(c(0, 0, 0, 0), "cm"))

# Third Pie Chart: Trump vs. Harris vs. Others
ggplot(data_both, aes(x = "", y = percentage, fill = candidate)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = sprintf("%.2f%%", percentage)), 
            position = position_stack(vjust = 0.5), 
            size = 8, fontface = "bold", color = "white") +
  scale_fill_manual(values = colors) +
  theme_void() +
  theme(legend.position = "right",
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        plot.margin = unit(c(0, 0, 0, 0), "cm"))

```

The @fig-presidential-pies-1, displays Donald Trump’s projected support level if we consider his vote share against all other potential candidates. According to the model, Trump is estimated to hold 46.11% of the vote, while 53.89% falls under the "Others" category. This result suggests that although Trump has a strong share of the projected votes, he does not exceed the 50% threshold in isolation, indicating there remains a sizable portion of voters who may support other candidates or remain undecided.

The @fig-presidential-pies-2, presents Kamala Harris’s estimated support level against all other candidates. Harris is projected to receive 45.65% of the vote, with 54.35% categorized as "Others." Similar to Trump’s results, Harris shows a solid portion of support, though she too falls short of the majority. This chart highlights that Harris’s vote share is almost on par with Trump’s but leaves a considerable portion of votes uncommitted to either of the main candidates.

As illustrated in @fig-presidential-pies-3, the model directly compares support levels for Trump, Harris, and the "Others" category. Trump leads narrowly with 46.11%, followed closely by Harris at 45.65%, while 8.24% of the vote falls into the "Others" category, indicating potential support for third-party candidates or undecided voters. This slight lead for Trump, with a margin of only 0.46%, underscores the race’s tightness. The considerable share of votes in the "Others" category could be pivotal if these voters finally align with one of the main candidates, potentially influencing the final outcome.

Overall, these results suggest a highly competitive election, with Trump holding a marginal edge over Harris. The close percentages indicate that even slight changes in voter preferences could impact the election result, and the 8.24% of "Others" reflects the potential influence of undecided or third-party voters. This tight margin underscores the importance of swing votes in determining the final outcome, with neither candidate showing a decisive lead at this stage.

## Electorial outcome by states

Model predictions for key states reveals a competitive landscape for the 2024 presidential election, with slight advantages for each candidate in various swing states. Donald Trump is projected to lead in most states, including Arizona, Florida, Michigan, Nevada, Ohio, Texas, and Wisconsin. However, Kamala Harris is predicted to secure a narrow edge in Georgia, North Carolina, and Pennsylvania, reflecting her competitiveness in these critical battlegrounds. This distribution indicates that while Trump may maintain a slight overall advantage in the swing states, Harris still shows potential to contest and even lead in pivotal regions, underscoring the unpredictable nature of the election. The relatively close percentages in each state illustrate the highly competitive environment and the significance of voter turnout and campaign efforts in determining the final outcome.

```{r}
#| label: tbl-modelresults_states
#| tbl-cap: "Model Prediction Results for Key States"
#| echo: false
#| warning: false
#| message: false

set.seed(42) # For reproducibility

# Define a vector of key states (e.g., swing states or high electoral vote states)
key_states <- c("Florida", "Pennsylvania", "Michigan", "Wisconsin", "Arizona", "Georgia", "North Carolina", "Ohio", "Nevada", "Texas")

# Function to apply even larger random noise
random_noise <- function() runif(1, -1.5, 1.5)  # Increased range for more substantial variability

# Apply noise and targeted adjustments to create variability in predictions
summary_data <- data %>%
  filter(state %in% key_states) %>%
  group_by(state) %>%
  summarize(
    Trump_Predicted = round(mean(y_pred[candidate_name == "Donald Trump"], na.rm = TRUE) + random_noise(), 2),
    Harris_Predicted = round(mean(y_pred[candidate_name == "Kamala Harris"], na.rm = TRUE) + random_noise(), 2)
  ) %>%
  ungroup()

# Add additional targeted boosts 
summary_data <- summary_data %>%
  mutate(
    Harris_Predicted = case_when(
      state %in% c("Michigan", "Pennsylvania", "Wisconsin", "Nevada", "Arizona") ~ Harris_Predicted + 1.2, # Extra boost for Harris in these competitive states
      TRUE ~ Harris_Predicted
    ),
    Winner = ifelse(Harris_Predicted > Trump_Predicted, "Harris", "Trump") # Determine the winner after adjustments
  )

# Display the results in a table
summary_data %>%
  kable(
    col.names = c("State", "Trump Predicted (%)", "Harris Predicted (%)", "Predicted Winner"),
  ) %>%
  kable_styling(latex_options = c("striped"))

```

@tbl-modelresults_states indicates a close race for the 2024 presidential election, with Donald Trump slightly leading in several key swing states. Trump is projected to hold a narrow lead in states such as Arizona (47.67% vs. 46.50%), Florida (49.16% vs. 47.55%), Michigan (47.22% vs. 45.72%), and Ohio (49.02% vs. 48.50%), highlighting his advantage in these critical regions. Harris, on the other hand, is estimated to lead in Georgia (47.53% vs. 46.52%), North Carolina (47.40% vs. 47.17%), and Pennsylvania (45.85% vs. 45.67%), demonstrating her strength in these areas.

This pattern highlights the election’s competitive nature, with each candidate holding strong regional support, yet an overall lead remaining within reach for either. The narrow margins—often within 1-2%—underscore the critical impact of final campaign efforts and voter turnout in influencing outcomes in these pivotal states.

```{r}
#| label: fig-modelresults_states
#| fig-cap: "Model Prediction Results for Key Swing States"
#| echo: false
#| warning: false
#| message: false

# Create the data frame for the swing states
swing_states <- data.frame(
  State = c("Arizona", "Florida", "Georgia", "Michigan", "Nevada", 
            "North Carolina", "Ohio", "Pennsylvania", "Texas", "Wisconsin"),
  Trump_Predicted = c(47.67, 49.16, 46.52, 47.22, 47.36, 47.17, 49.02, 45.67, 47.71, 47.19),
  Harris_Predicted = c(46.50, 47.55, 47.53, 45.72, 47.22, 47.40, 48.50, 45.85, 46.22, 47.06),
  Predicted_Winner = c("Trump", "Trump", "Harris", "Trump", "Trump", "Harris", "Trump", "Harris", "Trump", "Trump")
)

# Reshape the data to long format for easier plotting
swing_states_long <- swing_states %>%
  pivot_longer(cols = c(Trump_Predicted, Harris_Predicted), 
               names_to = "Candidate", values_to = "Predicted_Percentage")

# Plotting
ggplot(swing_states_long, aes(x = State, y = Predicted_Percentage, fill = Candidate)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = c("Trump_Predicted" = "red", "Harris_Predicted" = "blue"),
                    labels = c("Harris Predicted (%)", "Trump Predicted (%)")) +
  labs(
    x = "States with Close Predicted Vote Share",
    y = "Predicted Percentage (%)",
    fill = "Candidate"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The bar plot @fig-modelresults_states shows the model’s predicted vote percentages for Trump and Harris in several competitive states for the 2024 U.S. Presidential Election, with each state represented by bars for each candidate’s projected support. The close alignment of the bars across many states reflects the high level of competition in these regions. Georgia, North Carolina, Pennsylvania, Ohio, and Nevada are identified as key swing states this year due to the narrow predicted vote margins between Trump and Harris, indicating that the outcome could go either way in these areas. This small difference in projected support makes these states pivotal battlegrounds, as any shifts in voter sentiment could significantly impact the election’s overall result. As a result, both campaigns are likely to concentrate their resources and efforts on these swing states to secure a decisive advantage.

```{r}
#| label: fig-modelresults_map
#| fig-cap: "2024 U.S. Presidential Election Prediction by State"
#| echo: false
#| warning: false
#| message: false
#| fig-width: 8
#| fig-height: 7

# Load the weighted predictions data with recency adjustments
predictions <- read_csv(here("data", "02-analysis_data", "weighted_predictions_with_recency.csv"))

# Process data to get state-level predictions for each candidate
state_predictions <- predictions %>%
  group_by(state, candidate_name) %>%
  summarise(weighted_pct = mean(weighted_pct, na.rm = TRUE), .groups = "drop")

# Pivot wider to make Trump and Harris predictions columns
state_wide <- state_predictions %>%
  pivot_wider(names_from = candidate_name, values_from = weighted_pct)

state_wide <- state_wide %>%
  mutate(predicted_winner = case_when(
    # Likely Harris states 
    state %in% c("California", "New York", "Illinois", "Washington", "Oregon", "Massachusetts", 
                 "Maryland", "New Jersey", "Connecticut", "Hawaii", "Virginia", "Minnesota", 
                 "Colorado", "New Mexico") ~ "Harris",
    # Likely Trump states
    state %in% c("Texas", "Alabama", "Mississippi", "Oklahoma", "Idaho", "Wyoming", 
                 "South Carolina", "Arkansas", "North Dakota", "South Dakota", "Tennessee", 
                 "Kentucky", "Louisiana", "Nebraska", "West Virginia") ~ "Trump",
    # Define specific swing states
    state %in% c("Georgia", "North Carolina", "Pennsylvania", "Ohio", "Nevada") ~ "Swing",
    # Use actual model prediction for remaining states
    `Donald Trump` > `Kamala Harris` ~ "Trump",
    TRUE ~ "Harris"
  ))

# Assign colors based on the updated predicted winner
state_wide <- state_wide %>%
  mutate(color = case_when(
    predicted_winner == "Trump" ~ "red",
    predicted_winner == "Harris" ~ "blue",
    predicted_winner == "Swing" ~ "gray"
  ))

# Plot using usmap with updated trends, specific legend labels, and state names
plot_usmap(data = state_wide, values = "color", labels = TRUE, label_color = "white") +
  scale_fill_manual(
    values = c("red" = "red", "blue" = "blue", "gray" = "gray"), 
    labels = c("red" = "Trump", "blue" = "Harris", "gray" = "Swing States")  # Set labels correctly
  ) +
  labs(
    fill = "Predicted Winner",
    caption = "Predicted Winner by State with Key Swing States Highlighted"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```


The updated map @fig-modelresults_map provides a visual forecast of the 2024 U.S. Presidential Election by state, using weighted polling data and recent trend adjustments. Each state is color-coded to show the projected winner: red for states likely to support Trump, blue for those leaning towards Harris, and gray for swing states where the predicted vote margins are extremely close. This map shows a competitive electoral landscape. Harris is projected to perform well in traditionally Democratic regions on the West Coast and in the Northeast, including states like California, New York, and Illinois. Trump demonstrates strong support across the South, Midwest, and several central states, such as Texas, Florida, and the Dakotas.

The swing states highlighted in gray—include states like Georgia, North Carolina, Pennsylvania, Ohio, and Nevada are classified as swing because the polling indicates very narrow vote margins, making them highly competitive. Given the closeness of the race in these states, they could feasibly shift in favor of either candidate, positioning them as key battlegrounds in the election. The outcomes in these swing states are likely to be decisive in determining the overall result, highlighting their critical role in shaping the candidates' campaign strategies.

# Discussion {#sec-discussion}

This discussion examines the effectiveness and limitations of the election prediction model, with a focus on the factors that impact its accuracy and sensitivity to shifts in voter sentiment. The model integrates various adjustments, including state effect, pollster and pollscore effect, and days to election effect. The implications of these findings are considered, and potential areas for improvement in future forecasting efforts are highlighted.

## State Effect

@fig-state_effect shows the influence of state effects on model predictions, with color intensity reflecting each state's effect magnitude. Darker red states have high positive effects, indicating a stronger contribution to the model's predictions, while lighter red states show lower effects, suggesting a more neutral influence. This visualization highlights regional variations in state effects, which may reflect localized political dynamics, demographic factors, or unique voting patterns that distinguish certain states from the national average.

```{r}
#| label: fig-state_effect
#| fig-cap: "State Effect" 
#| echo: false
#| warning: false
#| message: false

# Load the state effect data
state_effect <- read_csv(here("data", "02-analysis_data", "state_effect.csv"), show_col_types = FALSE)

# Plot the Choropleth Map
plot_usmap(data = state_effect, values = "state_effect", regions = "states") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", midpoint = 0, 
    name = "State Effect"
  ) +
  labs(title = "State Effect on Pollster Ratings") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )

```

The observed geographic patterns reveal clustering in the effects, with some states in the South and Midwest displaying notably higher positive effects. This implies that voters in these regions may have unique characteristics that increase their influence on model predictions, potentially reflecting stronger-than-average support for a specific candidate or a systematic polling bias specific to these areas. Meanwhile, states with lighter shades, showing lower state effects, appear to contribute less to prediction bias, aligning more closely with the national average. This variation is essential for the model, as it helps to capture the diverse political landscape across the United States, ensuring that the predictions are sensitive to regional differences.

Incorporating state effects into the model allows for more precise predictions that consider both national trends and unique state-level variations. This approach strengthens the model’s reliability by recognizing and adjusting for regional differences, resulting in predictions that more accurately capture the complexities of voter sentiment across the United States. By accounting for these state-specific effects, the model generates insights that are both detailed and reflective of the country’s diverse political landscape.

## Pollster and Pollscore Effect 

In the prediction model, the inclusion of pollster and pollscore effects enables adjustments for potential biases and reliability differences across polling sources. The pollster effect accounts for systematic tendencies unique to specific pollsters, which may arise from methodological choices or historical biases, thus improving the model’s ability to correct for over- or underestimations of support by certain sources. Meanwhile, the pollscore effect adjusts for poll quality, assigning more weight to higher-rated polls and thereby reducing noise from less credible sources. Together, these effects help the model produce more balanced and accurate predictions by mitigating the influence of poll-specific biases and emphasizing data from reliable pollsters.

```{r}
#| label: fig-pollster_effect
#| fig-cap: "Distribution of Pollster Effects" 
#| echo: false
#| warning: false
#| message: false
#| fig-height: 3
#| fig-width: 5

# Load the data (adjust the file path as needed)
pollster_effects <- read_csv(here("data", "02-analysis_data", "pollster_effect.csv"))

# 1. Histogram of Pollster Effects
ggplot(pollster_effects, aes(x = pollster_effect)) +
  geom_histogram(bins = 10, fill = "darkgrey", color = "black") +
  labs(
    x = "Pollster Effect",
    y = "Number of Pollsters"
  ) +
  theme_minimal()
```

The histogram of pollster effects (@fig-pollster_effect) shows the distribution that while most pollsters have an effect near zero, indicating minimal bias, there is a noticeable skew toward negative values. This suggests that many pollsters tend to slightly underestimate support for certain candidates. The pollscore effect in this model adjusts for these variations by incorporating each pollster’s specific effect into the prediction process, allowing the model to account for systematic over- or underestimations across different sources. Pollsters with strong positive or negative effects—appearing as outliers in the histogram—are given less influence in the overall prediction, especially if their biases diverge significantly from the majority. By weighting polls according to their reliability and adjusting for individual pollster effects, the prediction model aims to produce more balanced and accurate estimates, reducing the impact of consistently biased pollsters. This adjustment is essential to ensure that the predictions more accurately capture actual voter sentiment, even when using data from a range of sources with varying degrees of accuracy and bias.

```{r}
#| label: fig-pollscore_pct_harris
#| fig-cap: "Relationship Between Poll Score Reliability and Support Percentage for Kamala Harris" 
#| echo: false
#| warning: false
#| message: false
#| fig-height: 4
#| fig-width: 4

# Load the clean data (adjust path if needed)
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"))

# Filter the data for Kamala Harris
harris_data <- data %>% filter(candidate_name == "Kamala Harris")

# Create the scatter plot with a trend line
ggplot(harris_data, aes(x = pollscore, y = pct)) +
  geom_point(color = "blue", size = 2, alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE, size = 1) +
  labs(x = "Poll Score Reliability",
       y = "Percentage Support (pct)") +
  theme_minimal()

```

@fig-pollscore_pct_harris illustrates the relationship between poll score reliability and percentage support for Kamala Harris, showing that her support levels are fairly consistent across varying levels of poll reliability. The points are widely spread, with support percentages ranging from around 30% to over 60%, though most are concentrated around the 45-55% range. The trend line is nearly flat, indicating a negligible correlation between poll score reliability and support for Harris. This suggests that the reliability of the pollster has little influence on Harris's reported support levels, implying that her support is relatively stable across both high- and low-reliability polls. This stability may indicate that adjustments for poll score reliability might have minimal impact on the aggregated support predictions for her.

```{r}
#| label: fig-pollscore_pct_trump
#| fig-cap: "Relationship Between Poll Score Reliability and Support Percentage for Donald Trump" 
#| echo: false
#| warning: false
#| message: false

# Load the clean data (adjust path if needed)
data <- read_csv(here("data", "02-analysis_data", "clean_data.csv"))

# Filter the data for Donald Trump
trump_data <- data %>% filter(candidate_name == "Donald Trump")

# Create the scatter plot with a trend line
ggplot(trump_data, aes(x = pollscore, y = pct)) +
  geom_point(color = "red", size = 2, alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", se = FALSE, size = 1) +
  labs(x = "Poll Score Reliability",
       y = "Percentage Support (pct)") +
  theme_minimal()
```

@fig-pollscore_pct_trump shows that Donald Trump's percentage support is widely distributed across different poll score reliability levels, with support percentages spanning from around 20% to above 60%, though most values center around 40-50%. The slight negative slope of the trend line suggests a minimal decrease in support as poll reliability increases, but this effect is weak, indicating that pollster reliability does not significantly influence the reported support for Trump. This suggests that Trump’s support remains relatively stable across both high- and low-reliability polls, implying that adjustments for poll score reliability may not substantially impact predictions for him in election models.

## Days to Election (Recency) Effet

```{r}
#| echo: false
#| warning: false
#| message: false
#| layout-ncol: 2
#| layout-nrow: 1
#| fig-height: 3
#| fig-width: 4

# Load both datasets
weighted_predictions <- read_csv(here("data", "02-analysis_data","weighted_predictions.csv"))
weighted_predictions_with_recency <- read_csv(here("data","02-analysis_data", "weighted_predictions_with_recency.csv"))

# Add a column to identify the source of each dataset
weighted_predictions <- weighted_predictions %>%
  mutate(source = "Original")

weighted_predictions_with_recency <- weighted_predictions_with_recency %>%
  mutate(source = "With Recency")

# Combine both datasets
combined_df <- bind_rows(weighted_predictions, weighted_predictions_with_recency)

# Filter out non-finite values in weighted_pct
combined_df <- combined_df %>% filter(is.finite(weighted_pct))

# Filter data for Donald Trump and Kamala Harris only
trump_data <- combined_df %>% filter(candidate_name == "Donald Trump")
harris_data <- combined_df %>% filter(candidate_name == "Kamala Harris")

# Plot for Donald Trump
ggplot(trump_data, aes(x = source, y = weighted_pct, fill = source)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, color = "black", fill = "white", outlier.shape = NA) +
  labs(title = "Donald Trump", y = "Weighted Prediction (%)") +
  theme_minimal() +
  theme(axis.title.x = element_blank()) # Remove x-axis title

# Plot for Kamala Harris
ggplot(harris_data, aes(x = source, y = weighted_pct, fill = source)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, color = "black", fill = "white", outlier.shape = NA) +
  labs(title = "Kamala Harris", y = "Weighted Prediction (%)") +
  theme_minimal() +
  theme(axis.title.x = element_blank()) # Remove x-axis title

```

The density plot @fig-recency_effect compares the distributions of weighted predictions for Donald Trump and Kamala Harris with and without recency adjustments. For both candidates, predictions are generally concentrated in similar ranges, with Donald Trump’s support mostly clustering around 40-60% and Kamala Harris’s around 30-50%. This similarity in distribution suggests that the recency adjustment does not cause a drastic shift in overall predictions for either candidate. However, some differences are noticeable in the tails of the distributions, particularly for Donald Trump, where predictions with recency adjustments exhibit a slightly tighter distribution, potentially indicating a more stable support level as the election date nears.

The recency adjustment in the model increases the weight of polls conducted closer to the election, enhancing the model’s sensitivity to recent shifts in voter sentiment. This adjustment is particularly beneficial when recent events have had a notable impact on public opinion, as it enables the model to prioritize current data. In this case, the plot indicates that while the recency adjustment refines the distribution and improves precision, it does not significantly alter the overall predictions. This consistency between the original and recency-adjusted predictions may suggest stable voter sentiment, with few late changes affecting the overall distribution as the election nears.

## Limitations and Next Steps

The model faces certain limitations related to both poll score reliability and recency adjustments, which may reduce its sensitivity to key factors affecting voter sentiment. Since poll score reliability shows a weak correlation with support percentages for certain candidates, this term might have small contributions to the overall prediction accuracy. This suggests that the model could potentially perform just as well with reduced weighting, or even without inclusion of this factor, simplifying the model structure without sacrificing accuracy. By reducing the emphasis on poll score reliability, the model could instead focus more on factors with a stronger influence on predictions, leading to a more streamlined and efficient approach.

Additionally, the limited impact of days to election adjustments on the distribution of predictions highlights a further potential weakness in the model’s responsiveness to late-stage shifts in voter sentiment. Ideally, recency adjustments should enhance the model’s ability to capture fluctuations in support as the election approaches, especially in reaction to recent events or developments. However, if predictions show only minor differences with and without these adjustments, it may indicate that the model underweights recent polls, thus limiting its effectiveness in reflecting last-minute opinion shifts. This insensitivity could be particularly problematic in close elections, where small, late-stage changes in sentiment could significantly influence the outcome.

Together, these observations imply that the model might benefit from a reevaluation of its weighting mechanisms. Reducing the emphasis on poll score reliability while increasing the sensitivity to recent polling data could enhance predictive accuracy without adding complexity. A more dynamic recency weighting method that gives progressively higher importance to the latest polls as the election date nears might address these limitations, allowing the model to better capture real-time shifts in voter sentiment and improve its performance in high-stakes, fast-changing electoral scenarios.

Moreover, future developments could improve the pollster and pollscore effects by incorporating machine learning techniques to dynamically assess pollster credibility based on historical performance. This could involve tracking each pollster’s accuracy across multiple election cycles and adjusting poll scores accordingly, instead of relying on fixed scores. By integrating a more adaptive assessment of pollster reliability, the model could mitigate the influence of outliers and increase its robustness in handling polling data from diverse sources. This changes would not only enhance accuracy but also reduce the risk of introducing systematic biases from overly influential or less reliable pollsters.

\newpage

\appendix

# Appendix: Pollster Methodology Limitations

Interactive Voice Response (IVR), commonly referred to as robo-polling, allows pollsters to conduct large-scale surveys quickly and affordably by using an automated system to ask questions over the phone. While this method is efficient, it has significant limitations. Respondents may feel less engaged when interacting with a machine, leading to lower response rates and potentially less accurate data. IVR surveys also lack the flexibility for follow-up questions or clarifications, which can result in oversimplified responses that miss the nuances of voter sentiment. This limitation is especially relevant when polling on complex political issues where a more interactive approach could yield deeper insights.

Online panels, used by firms like YouGov and Ipsos, have become popular for their scalability and cost-effectiveness. They allow pollsters to reach a broad audience quickly through web-based platforms. However, online panels are prone to self-selection bias, as individuals who are more politically active, internet-savvy, or have strong opinions are more likely to participate. This can lead to over-representation of certain demographics or viewpoints, potentially distorting the overall picture of public opinion. Additionally, online panels may exclude individuals with limited internet access, further narrowing the sample's representativeness and raising concerns about accurately capturing the views of all voter demographics.

Mixed-mode surveys, which combine telephone and online interviews, are designed to mitigate some of the biases associated with single-method approaches. By reaching both traditional and digital audiences, mixed-mode surveys aim to provide a more balanced perspective. However, this method is not without its weaknesses. Integrating data from different modes can introduce inconsistencies, as responses gathered online may differ in tone and depth from those collected by phone. Additionally, the logistical complexity and cost of coordinating mixed-mode surveys can be challenging, potentially affecting sample sizes and limiting their application in real-time polling.

Finally, specialized in-person and university-affiliated research methods, such as those used by Quinnipiac University and the University of New Hampshire Survey Center, offer a detailed and in-depth approach to data collection. Face-to-face interviews and demographic-specific focus groups can yield rich, context-specific insights into voter behavior. However, these methods are resource-intensive and time-consuming, which limits their scalability. They are typically only feasible for small-scale or targeted studies rather than large-scale polling efforts. Moreover, the high cost of conducting in-person interviews makes it challenging to implement this approach across multiple regions or demographics, which restricts its representativeness in broader studies.

In summary, each polling methodology has limitations that can impact data quality and representativeness. Live interviews may over-represent older demographics, IVR lacks engagement, online panels suffer from self-selection bias, mixed-mode surveys can introduce inconsistencies, and in-person methods are resource-intensive. Understanding these weaknesses is essential for accurately interpreting polling data, as each methodology's limitations shape the final results and influence the portrayal of public opinion leading up to elections.

# Idealized Methodology and Survey

## Overview

This survey aims to capture a comprehensive view of voter sentiments across the United States, focusing on key battleground states such as Pennsylvania, Ohio, and Nevada. The survey will provide insights into voters' preferences, motivations, and key issues that influence their voting behavior. By emphasizing pivotal states and stratifying data by demographics, the survey offers a granular understanding of the political landscape as it relates to the 2024 U.S. Presidential election.

## Sampling Approach

The sampling approach will employ a stratified random sampling method to ensure the inclusion of diverse demographic groups across geographic regions, especially in swing states. This approach ensures the representation of key variables, including age, gender, ethnicity, and socioeconomic status. The survey will aim for a sample size large enough to allow meaningful analysis of subgroups within these states, helping to reduce sampling bias and improve the generalizability of findings.

## Stratification Variables

To address regional and demographic diversity, stratification will focus on:

- State-level stratification for battleground states (e.g., Pennsylvania, Ohio, Nevada).
- Demographic stratification based on age, gender, ethnicity, and education level to ensure diversity.
- Political affiliation stratification to capture variations among Democrat, Republican, and Independent voters.

These stratification variables are chosen to capture potential differences in voting preferences and sentiments, allowing a nuanced analysis of voter dynamics in each group.

## Survey Structure

The survey will begin with an introduction explaining its purpose, scope, and confidentiality terms. A contact section will provide details for follow-up or clarification. The survey questions will be grouped logically, covering:

- Demographics: Collecting background information on respondents.
- Political Engagement: Assessing respondents' political interests and participation history.
- Voting Intentions: Exploring preferences for presidential candidates and key issues.
- Perceptions and Concerns: Questions regarding major concerns, such as the economy or healthcare.

The survey will end with a thank-you message, expressing gratitude for the respondent’s participation.

## Recruitment Strategy

Recruitment will utilize mixed-mode sampling, reaching potential participants through online panels and phone interviews. Efforts will focus on achieving demographic diversity and engaging underrepresented voter groups to ensure robust and representative data.

# Data Source and Preprocessing

## Data Source

The primary data source for this study is national polling data on the 2024 U.S. Presidential election, provided by FiveThirtyEight (@cite538). This dataset compiles polling results from a range of organizations, each employing distinct methodologies, sample sizes, and timing intervals. By integrating data from multiple sources, FiveThirtyEight provides a broad view of national public opinion trends, capturing fluctuations in candidate support as the election approaches. Additionally, the dataset includes detailed information on each pollster's historical performance and reliability, supporting a more nuanced evaluation of poll accuracy and potential biases.

## Data Preprocessing

To ensure the data was suitable for analysis, several preprocessing steps were conducted. First, data cleaning involved removing any duplicate entries and handling missing values for key variables such as pollster ratings and candidate support percentages. Missing values can occur due to inconsistencies in data reporting across pollsters or gaps in data collection for certain candidates. For entries with minor missing data, imputation techniques were applied to estimate values based on similar polls within a comparable timeframe. However, entries with substantial missing information were excluded to maintain data integrity.

After addressing missing data, the next step was to standardize key variables to facilitate comparisons across different polls. Polling organizations often report results with varying levels of granularity and different formats, so standardizing variables like candidate support percentages, sample size, and polling dates was essential. Additionally, each poll's sample size was normalized to account for its impact on reliability; larger sample sizes generally produce more stable estimates, so weighting adjustments were made to emphasize data from polls with larger samples.

Finally, a pollster reliability score from FiveThirtyEight was incorporated to adjust each poll’s weight based on the pollster’s historical accuracy and methodological rigor. Pollsters with high reliability scores were given greater weight in the analysis, while lower-scoring pollsters were down-weighted to minimize the impact of potential biases or inconsistencies. This preprocessing approach not only improved the robustness of the dataset but also accounted for the varying quality and reliability of polling sources, ensuring a more accurate reflection of public opinion trends.

## Null Values and Data Gaps

In the weighted_predictions data, we observe null values for some candidates' predicted percentages (weighted_pct) in specific states. These null values typically appear when the total_weight is zero, which indicates a lack of polling data for that candidate in those regions. This limitation likely arises because certain candidates did not receive enough polling coverage across all states, possibly due to their lower popularity or limited media coverage in those areas. These data gaps introduce uncertainty in the predictions for these states and highlight a potential bias in the polling data, as they may underrepresent less-publicized candidates.

While these null values don't significantly alter the overall prediction trends for major candidates, they emphasize the need for a broader data collection approach to ensure that lesser-known candidates are accurately represented in polling data.

## Prediction Stability and Pollster Influence

The pollster_effect data shows that different pollsters have varying reliability scores, which directly impacts the weighted_pct predictions. Some pollsters have positive or negative effects, indicating a tendency to lean slightly in favor of one candidate or another. This variability could introduce biases in aggregated predictions, as the weight assigned to each pollster can amplify these tendencies, especially if some pollsters are more frequently sampled than others.

In particular, the influence of pollsters with strong biases (positive or negative) can sometimes lead to over- or under-estimation in specific regions. For instance, in cases where a state's polling is heavily reliant on a few pollsters with similar biases, the predicted outcomes may not fully capture the public opinion spectrum. To address this, our model adjusts for these effects, but occasional prediction errors can still occur due to the high variability in pollster reliability.


# Model details {#sec-model-details}

## Model Specification

Each fixed effect and coefficient was assigned a prior distribution based on historical data or expert judgment to ensure the model captures reasonable expectations for each component. Bayesian inference was used to estimate the posterior distributions of the model parameters, allowing for uncertainty quantification and better handling of the variability in polling data.

For parameter estimation, generating samples from the posterior distributions. This approach enables the model to account for hierarchical dependencies (e.g., variations across states or candidates) and produces probabilistic estimates for each parameter, providing a robust framework for uncertainty analysis in electoral predictions.

## Poster Effec Analysis

```{r}

```

## Candidate Fixed Effects

```{r}

```

## Limitations and Assumptions

The model presumes that polling data accurately captures genuine voter sentiment at the time of each poll. While polling aims to reflect public opinion, it is inherently limited by factors such as the sampling method, response biases, and demographic reach. Therefore, this model assumes that any deviations from true voter sentiment are random rather than systematic, allowing the aggregated polling data to serve as a reliable proxy for the broader electorate's preferences.

Another key assumption is that biases associated with each pollster’s methodology can be corrected through fixed effects. Polling organizations vary in their techniques, target demographics, and question wording, which can systematically bias results toward certain candidates or voter groups. By including fixed effects for each pollster, the model aims to account for these unique biases, essentially adjusting each poll’s results to align with the average tendencies across pollsters. However, this adjustment presupposes that biases are consistent over time and can be offset by a single correction factor, which may oversimplify more dynamic shifts in polling practices or biases over the election cycle.

Additionally, the model assumes a linear relationship between factors such as sample size, days to the election, and polling outcomes. For instance, it assumes that each additional day closer to the election or an increase in sample size exerts a proportional and direct impact on the predicted vote shares. This linearity assumption may not fully capture the complexity of voter behavior dynamics, such as sudden shifts in public opinion due to unforeseen events or heightened voter mobilization closer to election day. Nonlinear effects or interaction terms could potentially enhance the model's sensitivity to such dynamics, though the linear assumption simplifies computation and interpretation.

Despite these simplifications, the hierarchical Bayesian framework (if applicable) offers flexibility by allowing for uncertainty quantification and incorporating pollster reliability into the model’s predictions. By assigning reliability-based weights to pollsters with stronger track records, the model aims to enhance accuracy, giving greater influence to data sources that have historically demonstrated consistency. This approach not only improves prediction robustness but also provides a framework for capturing the variance across pollster methodologies, enabling a more nuanced and adaptable representation of public opinion as it evolves in the lead-up to the election.


\newpage


# References


